@inproceedings{adebayoSanityChecksSaliency2018a,
  title = {Sanity {{Checks}} for {{Saliency Maps}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html},
  urldate = {2023-10-12},
  abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
  file = {C:\Users\c21012241\Zotero\storage\VWBHWMXW\Adebayo et al. - 2018 - Sanity Checks for Saliency Maps.pdf}
}

@online{alainUnderstandingIntermediateLayers2018,
  title = {Understanding Intermediate Layers Using Linear Classifier Probes},
  author = {Alain, Guillaume and Bengio, Yoshua},
  date = {2018-11-22},
  eprint = {1610.01644},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1610.01644},
  urldate = {2023-08-24},
  abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as “probes”, trained entirely independently of the model itself.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\IVTM32IT\Alain and Bengio - 2018 - Understanding intermediate layers using linear cla.pdf}
}

@article{alkaissiArtificialHallucinationsChatGPT2023,
  title = {Artificial {{Hallucinations}} in {{ChatGPT}}: {{Implications}} in {{Scientific Writing}}},
  shorttitle = {Artificial {{Hallucinations}} in {{ChatGPT}}},
  author = {Alkaissi, Hussam and McFarlane, Samy I},
  date = {2023-02-19},
  journaltitle = {Cureus},
  issn = {2168-8184},
  doi = {10.7759/cureus.35179},
  url = {https://www.cureus.com/articles/138667-artificial-hallucinations-in-chatgpt-implications-in-scientific-writing},
  urldate = {2023-09-12},
  abstract = {While still in its infancy, ChatGPT (Generative Pretrained Transformer), introduced in November 2022, is bound to hugely impact many industries, including healthcare, medical education, biomedical research, and scientific writing. Implications of ChatGPT, that new chatbot introduced by OpenAI on academic writing, is largely unknown. In response to the Journal of Medical Science (Cureus) Turing Test - call for case reports written with the assistance of ChatGPT, we present two cases one of homocystinuria-associated osteoporosis, and the other is on late-onset Pompe disease (LOPD), a rare metabolic disorder. We tested ChatGPT to write about the pathogenesis of these conditions. We documented the positive, negative, and rather troubling aspects of our newly introduced chatbot’s performance.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\BIV29NBE\Alkaissi and McFarlane - 2023 - Artificial Hallucinations in ChatGPT Implications.pdf}
}

@online{amarasingheImportanceApplicationGroundedExperimental2023,
  title = {On the {{Importance}} of {{Application-Grounded Experimental Design}} for {{Evaluating Explainable ML Methods}}},
  author = {Amarasinghe, Kasun and Rodolfa, Kit T. and Jesus, Sérgio and Chen, Valerie and Balayan, Vladimir and Saleiro, Pedro and Bizarro, Pedro and Talwalkar, Ameet and Ghani, Rayid},
  date = {2023-02-21},
  eprint = {2206.13503},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.13503},
  urldate = {2023-08-27},
  abstract = {Most existing evaluations of explainable machine learning (ML) methods rely on simplifying assumptions or proxies that do not reflect real-world use cases; the handful of more robust evaluations on real-world settings have shortcomings in their design, resulting in limited conclusions of methods' real-world utility. In this work, we seek to bridge this gap by conducting a study that evaluates three popular explainable ML methods in a setting consistent with the intended deployment context. We build on a previous study on e-commerce fraud detection and make crucial modifications to its setup relaxing the simplifying assumptions made in the original work that departed from the deployment context. In doing so, we draw drastically different conclusions from the earlier work and find no evidence for the incremental utility of the tested methods in the task. Our results highlight how seemingly trivial experimental design choices can yield misleading conclusions, with lessons about the necessity of not only evaluating explainable ML methods using tasks, data, users, and metrics grounded in the intended deployment contexts but also developing methods tailored to specific applications. In addition, we believe the design of this experiment can serve as a template for future study designs evaluating explainable ML methods in other real-world contexts.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\8QWUI7MZ\Amarasinghe et al. - 2023 - On the Importance of Application-Grounded Experime.pdf}
}

@article{apukeFakeNewsCOVID192021,
  title = {Fake News and {{COVID-19}}: Modelling the Predictors of Fake News Sharing among Social Media Users},
  shorttitle = {Fake News and {{COVID-19}}},
  author = {Apuke, Oberiri Destiny and Omar, Bahiyah},
  date = {2021-01},
  journaltitle = {Telematics and Informatics},
  shortjournal = {Telematics and Informatics},
  volume = {56},
  pages = {101475},
  issn = {07365853},
  doi = {10.1016/j.tele.2020.101475},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0736585320301349},
  urldate = {2023-08-29},
  abstract = {Fake news dissemination on COVID-19 has increased in recent months, and the factors that lead to the sharing of this misinformation is less well studied. Therefore, this paper describes the result of a Nigerian sample (n = 385) regarding the proliferation of fake news on COVID-19. The fake news phenomenon was studied using the Uses and Gratification framework, which was extended by an “altruism” motivation. The data were analysed with Partial Least Squares (PLS) to de­ termine the effects of six variables on the outcome of fake news sharing. Our results showed that altruism was the most significant factor that predicted fake news sharing of COVID-19. We also found that social media users’ motivations for information sharing, socialisation, information seeking and pass time predicted the sharing of false information about COVID-19. In contrast, no sig­ nificant association was found for entertainment motivation. We concluded with some theoretical and practical implications.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\VS7Z6S3V\Apuke and Omar - 2021 - Fake news and COVID-19 modelling the predictors o.pdf}
}

@article{ashbyNeuropsychologicalTheoryMultiple1998,
  title = {A {{Neuropsychological Theory}} of {{Multiple Systems}} in {{Category Learning}}},
  author = {Ashby, Gregory and Alfonso-Reese, Leola A and Turken, U and Waldron, Elliott M},
  date = {1998},
  journaltitle = {Psychological Review},
  volume = {105},
  number = {3},
  pages = {442},
  doi = {https://psycnet.apa.org/doi/10.1037/0033-295X.105.3.442},
  abstract = {A neuropsychological theory is proposed that assumes category learning is a competition between separate verbal and implicit (i.e., procedural-learning-based) categorization systems. The theory assumes that the caudate nucleus is an important component of the implicit system and that the anterior cingulate and prefrontal cortices are critical to the verbal system. In addition to making predictions for normal human adults, the theory makes specific predictions for children, elderly people, and patients suffering from Parkinson's disease, Huntington's disease, major depression, amnesia, or lesions of the prefrontal cortex. Two separate formal descriptions of the theory are also provided. One describes trial-by-trial learning, and the other describes global dynamics. The theory is tested on published neuropsychological data and on category learning data with normal adults.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\BLJI2XBI\Ashby et al. - A Neuropsychological Theory of Multiple Systems in.pdf}
}

@online{attewellExploringRoleGenerative2023,
  title = {Exploring the Role of Generative {{AI}} in Assessments: {{A}} Student Perspective},
  shorttitle = {Exploring the Role of Generative {{AI}} in Assessments},
  author = {Attewell, Sue},
  date = {2023-06-14T10:35:48+00:00},
  url = {https://nationalcentreforai.jiscinvolve.org/wp/2023/06/14/exploring-the-role-of-generative-ai-in-assessments-a-student-perspective/},
  urldate = {2023-08-29},
  abstract = {Recently, we organised five discussion forums for tertiary education students on Generative AI to understand how students are currently using this technology and explore its potential impact on their learning experience. This blog delves student perspectives on AI and assessment,},
  langid = {british},
  organization = {{National centre for AI}},
  file = {C:\Users\c21012241\Zotero\storage\9S4QUCWC\exploring-the-role-of-generative-ai-in-assessments-a-student-perspective.html}
}

@online{balayanTeachingMachineExplain2020,
  title = {Teaching the {{Machine}} to {{Explain Itself}} Using {{Domain Knowledge}}},
  author = {Balayan, Vladimir and Saleiro, Pedro and Belém, Catarina and Krippahl, Ludwig and Bizarro, Pedro},
  date = {2020-11-27},
  eprint = {2012.01932},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.01932},
  urldate = {2023-08-06},
  abstract = {Machine Learning (ML) has been increasingly used to aid humans to make better and faster decisions. However, non-technical humans-in-the-loop struggle to comprehend the rationale behind model predictions, hindering trust in algorithmic decision-making systems. Considerable research work on AI explainability attempts to win back trust in AI systems by developing explanation methods but there is still no major breakthrough. At the same time, popular explanation methods (e.g., LIME, and SHAP) produce explanations that are very hard to understand for nondata scientist persona. To address this, we present JOEL, a neural network-based framework to jointly learn a decision-making task and associated explanations that convey domain knowledge. JOEL is tailored to human-in-the-loop domain experts that lack deep technical ML knowledge, providing high-level insights about the model’s predictions that very much resemble the experts’ own reasoning. Moreover, we collect the domain feedback from a pool of certified experts and use it to ameliorate the model (human teaching), hence promoting seamless and better suited explanations. Lastly, we resort to semantic mappings between legacy expert systems and domain taxonomies to automatically annotate a bootstrap training set, overcoming the absence of concept-based human annotations. We validate JOEL empirically on a real-world fraud detection dataset. We show that JOEL can generalize the explanations from the bootstrap dataset. Furthermore, obtained results indicate that human teaching can further improve the explanations prediction quality by approximately 13.57\%.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2},
  file = {C:\Users\c21012241\Zotero\storage\84K6MP76\Balayan et al. - 2020 - Teaching the Machine to Explain Itself using Domai.pdf}
}

@article{barredoarrietaExplainableArtificialIntelligence2020,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  date = {2020-06},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {15662535},
  doi = {10.1016/j.inffus.2019.12.012},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
  urldate = {2023-08-25},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\EYURPMVP\Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf}
}

@inproceedings{begumDeepLearningBasedLung2023,
  title = {Deep {{Learning-Based Lung Cancer Classification}}: {{Recent Developments}} and {{Future Prospects}}},
  shorttitle = {Deep {{Learning-Based Lung Cancer Classification}}},
  booktitle = {2023 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communication}} and {{Applied Informatics}} ({{ACCAI}})},
  author = {Begum, Almas and Alex David, S and Hemalatha, D and Kollipara, Lavanya Sita Sai},
  date = {2023-05-25},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Chennai, India}},
  doi = {10.1109/ACCAI58221.2023.10200967},
  url = {https://ieeexplore.ieee.org/document/10200967/},
  urldate = {2023-08-15},
  abstract = {This article discusses about potential of deep learning in lung cancer diagnosis using medical imaging, specifically focusing on the performance of various deep learning models in accurately identifying lung nodules from CT scans. Based on the findings, it was revealed that various deep learning algorithms, mainly deals with convolutional neural networks, demonstrate a remarkable degree of accuracy and an appropriate trade-off between sensitivity and specificity in correctly identifying true positives and true negatives. However, there is a need for more detailed analysis and comparison of these methods to fully understand their performance characteristics. The article also identifies several potential future scopes of deep learning in medical imaging and lung cancer diagnosis, including integrating deep learning models into clinical practice, large-scale multi-center studies to validate the effectiveness of deep learningbased approaches, improving the interpretability of deep learning models, integrating information from multiple imaging modalities, and implementing deep learning-based approaches in screening programs to improve early detection and patient outcomes. Overall, the results suggest that deep learning has significant potential to improve lung cancer diagnosis and treatment, but further research is needed to fully realize this potential.},
  eventtitle = {2023 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communication}} and {{Applied Informatics}} ({{ACCAI}})},
  isbn = {9798350315905},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\2GS67DNX\Begum et al. - 2023 - Deep Learning-Based Lung Cancer Classification Re.pdf}
}

@article{bennWhatWrongAutomated2022,
  title = {What’s {{Wrong}} with {{Automated Influence}}},
  author = {Benn, Claire and Lazar, Seth},
  date = {2022-01},
  journaltitle = {Canadian Journal of Philosophy},
  shortjournal = {Can. J. of Philosophy},
  volume = {52},
  number = {1},
  pages = {125--148},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2021.23},
  url = {https://www.cambridge.org/core/product/identifier/S0045509121000230/type/journal_article},
  urldate = {2023-10-09},
  abstract = {Automated Influence is the use of Artificial Intelligence (AI) to collect, integrate, and analyse people’s data in order to deliver targeted interventions that shape their behaviour. We consider three central objections against Automated Influence, focusing on privacy, exploitation, and manipulation, showing in each case how a structural version of that objection has more purchase than its interactional counterpart. By rejecting the interactional focus of “AI Ethics” in favour of a more structural, political philosophy of AI, we show that the real problem with Automated Influence is the crisis of legitimacy that it precipitates.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\ALCCSK9G\Benn and Lazar - 2022 - What’s Wrong with Automated Influence.pdf}
}

@article{bhosekarAdvancesSurrogateBased2018,
  title = {Advances in Surrogate Based Modeling, Feasibility Analysis, and Optimization: {{A}} Review},
  shorttitle = {Advances in Surrogate Based Modeling, Feasibility Analysis, and Optimization},
  author = {Bhosekar, Atharv and Ierapetritou, Marianthi},
  date = {2018-01},
  journaltitle = {Computers \& Chemical Engineering},
  shortjournal = {Computers \& Chemical Engineering},
  volume = {108},
  pages = {250--267},
  issn = {00981354},
  doi = {10.1016/j.compchemeng.2017.09.017},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135417303228},
  urldate = {2023-08-15},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\VPRXRZHA\Bhosekar and Ierapetritou - 2018 - Advances in surrogate based modeling, feasibility .pdf}
}

@article{cabitzaQuodEratDemonstrandum2023,
  title = {Quod Erat Demonstrandum? - {{Towards}} a Typology of the Concept of Explanation for the Design of Explainable {{AI}}},
  shorttitle = {Quod Erat Demonstrandum?},
  author = {Cabitza, Federico and Campagner, Andrea and Malgieri, Gianclaudio and Natali, Chiara and Schneeberger, David and Stoeger, Karl and Holzinger, Andreas},
  date = {2023-03},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {213},
  pages = {118888},
  issn = {09574174},
  doi = {10.1016/j.eswa.2022.118888},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422019066},
  urldate = {2023-08-27},
  abstract = {In this paper, we present a fundamental framework for defining different types of explanations of AI systems and the criteria for evaluating their quality. Starting from a structural view of how explanations can be constructed, i.e., in terms of an explanandum (what needs to be explained), multiple explanantia (explanations, clues, or parts of information that explain), and a relationship linking explanandum and explanantia, we propose an explanandum-based typology and point to other possible typologies based on how explanantia are presented and how they relate to explanandia. We also highlight two broad and complementary perspectives for defining possible quality criteria for assessing explainability: epistemological and psychological (cognitive). These definition attempts aim to support the three main functions that we believe should attract the interest and further research of XAI scholars: clear inventories, clear verification criteria, and clear validation methods.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\YJI7KRCC\Cabitza et al. - 2023 - Quod erat demonstrandum - Towards a typology of t.pdf}
}

@online{ChatGPT,
  title = {{{ChatGPT}}},
  url = {https://chat.openai.com},
  urldate = {2023-09-12},
  abstract = {A conversational AI system that listens, learns, and challenges},
  file = {C:\Users\c21012241\Zotero\storage\KA39D9JR\chat.openai.com.html}
}

@article{chenConceptWhiteningInterpretable2020,
  title = {Concept {{Whitening}} for {{Interpretable Image Recognition}}},
  author = {Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
  date = {2020-12-07},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {2},
  number = {12},
  eprint = {2002.01650},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {772--782},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00265-z},
  url = {http://arxiv.org/abs/2002.01650},
  urldate = {2023-08-17},
  abstract = {What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging to understand. Attempts to see inside their hidden layers can either be misleading, unusable, or rely on the latent space to possess properties that it may not have. In this work, rather than attempting to analyze a neural network posthoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us to better understand the computation leading up to that layer. When a concept whitening module is added to a CNN, the axes of the latent space are aligned with known concepts of interest. By experiment, we show that CW can provide us a much clearer understanding for how the network gradually learns concepts over layers. CW is an alternative to a batch normalization layer in that it normalizes, and also decorrelates (whitens) the latent space. CW can be used in any layer of the network without hurting predictive performance.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\M9FYE5RW\Chen et al. - 2020 - Concept Whitening for Interpretable Image Recognit.pdf}
}

@inproceedings{chengFlockHybridCrowdMachine2015,
  title = {Flock: {{Hybrid Crowd-Machine Learning Classifiers}}},
  shorttitle = {Flock},
  booktitle = {Proceedings of the 18th {{ACM Conference}} on {{Computer Supported Cooperative Work}} \& {{Social Computing}}},
  author = {Cheng, Justin and Bernstein, Michael S.},
  date = {2015-02-28},
  pages = {600--611},
  publisher = {{ACM}},
  location = {{Vancouver BC Canada}},
  doi = {10.1145/2675133.2675214},
  url = {https://dl.acm.org/doi/10.1145/2675133.2675214},
  urldate = {2023-08-24},
  abstract = {We present hybrid crowd-machine learning classifiers: classification models that start with a written description of a learning goal, use the crowd to suggest predictive features and label data, and then weigh these features using machine learning to produce models that are accurate and use humanunderstandable features. These hybrid classifiers enable fast prototyping of machine learning models that can improve on both algorithm performance and human judgment, and accomplish tasks where automated feature extraction is not yet feasible. Flock, an interactive machine learning platform, instantiates this approach. To generate informative features, Flock asks the crowd to compare paired examples, an approach inspired by analogical encoding. The crowd’s efforts can be focused on specific subsets of the input space where machine-extracted features are not predictive, or instead used to partition the input space and improve algorithm performance in subregions of the space. An evaluation on six prediction tasks, ranging from detecting deception to differentiating impressionist artists, demonstrated that aggregating crowd features improves upon both asking the crowd for a direct prediction and off-the-shelf machine learning features by over 10\%. Further, hybrid systems that use both crowd-nominated and machine-extracted features can outperform those that use either in isolation.},
  eventtitle = {{{CSCW}} '15: {{Computer Supported Cooperative Work}} and {{Social Computing}}},
  isbn = {978-1-4503-2922-4},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\Y9XPU7MI\Cheng and Bernstein - 2015 - Flock Hybrid Crowd-Machine Learning Classifiers.pdf}
}

@online{coldeweyBulgariaNowRequires2016,
  title = {Bulgaria Now Requires (Some) Government Software to Be Open Source},
  author = {Coldewey, Devin},
  date = {2016-07-06T00:33:19+00:00},
  url = {https://techcrunch.com/2016/07/05/bulgaria-now-requires-some-government-software-to-be-open-source/},
  urldate = {2023-08-24},
  abstract = {Fans of free and open-source software are rejoicing today at the news that Bulgaria will now require all software written for the government must be FOSS. But while this is a promising advance, don't expect a major change in the way things work.},
  langid = {american},
  organization = {{TechCrunch}},
  file = {C:\Users\c21012241\Zotero\storage\M7Z8FWWJ\bulgaria-now-requires-some-government-software-to-be-open-source.html}
}

@inproceedings{collinsHumanUncertaintyConceptBased2023,
  title = {Human {{Uncertainty}} in {{Concept-Based AI Systems}}},
  booktitle = {Proceedings of the 2023 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Collins, Katherine Maeve and Barker, Matthew and Espinosa Zarlenga, Mateo and Raman, Naveen and Bhatt, Umang and Jamnik, Mateja and Sucholutsky, Ilia and Weller, Adrian and Dvijotham, Krishnamurthy},
  date = {2023-08-08},
  pages = {869--889},
  publisher = {{ACM}},
  location = {{Montr\textbackslash '\{e\}al QC Canada}},
  doi = {10.1145/3600211.3604692},
  url = {https://dl.acm.org/doi/10.1145/3600211.3604692},
  urldate = {2023-10-04},
  abstract = {Placing a human in the loop may help abate the risks of deploying AI systems in safety-critical settings ( e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, UElic, to collect uncertain feedback from humans in collaborative prediction tasks.},
  eventtitle = {{{AIES}} '23: {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  isbn = {9798400702310},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\6K8SMJ72\Collins et al. - 2023 - Human Uncertainty in Concept-Based AI Systems.pdf}
}

@online{DALL,
  title = {{{DALL}}·{{E}} 2},
  url = {https://openai.com/dall-e-2},
  urldate = {2023-09-12},
  file = {C:\Users\c21012241\Zotero\storage\X2MXSCX8\dall-e-2.html}
}

@inproceedings{daneshjouSkinConSkinDisease2022,
  title = {{{SkinCon}}: {{A}} Skin Disease Dataset Densely Annotated by Domain Experts for Fine-Grained Model Debugging and Analysis},
  booktitle = {Advances in {{Neural Information Processing Systems}} 35 ({{NeurIPS}} 2022)},
  author = {Daneshjou, Roxana and Yuksekgonul, Mert and Cai, Zhuo Ran and Novoa, Roberto and Zou, James},
  date = {2022},
  doi = {10.48550/arXiv.2302.00785},
  abstract = {For the deployment of artificial intelligence (AI) in high risk settings, such as healthcare, methods that provide interpretability/explainability or allow fine-grained error analysis are critical. Many recent methods for interpretability/explainability and fine-grained error analysis use concepts, which are meta-labels that are semantically meaningful to humans. However, there are only a few datasets that include concept-level meta-labels and most of these meta-labels are relevant for natural images that do not require domain expertise. Previous densely annotated datasets in medicine focused on meta-labels that are relevant to a single disease such as osteoarthritis or melanoma. In dermatology, skin disease is described using an established clinical lexicon that allows clinicians to describe physical exam findings to one another. To provide a medical dataset densely annotated by domain experts with annotations useful across multiple disease processes, we developed SkinCon: a skin disease dataset densely annotated by dermatologists. SkinCon includes 3230 images from the Fitzpatrick 17k skin disease dataset densely annotated with 48 clinical concepts, 22 of which have at least 50 images representing the concept. The concepts used were chosen by two dermatologists considering the clinical descriptor terms used to describe skin lesions. Examples include "plaque", "scale", and "erosion". These same concepts were also used to label 656 skin disease images from the Diverse Dermatology Images dataset, providing an additional external dataset with diverse skin tone representations. We review the potential applications for the SkinCon dataset, such as probing models, concept-based explanations, concept bottlenecks, error analysis, and slice discovery. Furthermore, we use SkinCon to demonstrate two of these use cases: debugging mistakes of an existing dermatology AI model with concepts and developing interpretable models with post-hoc concept bottleneck models.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\JTNI5H6Q\Daneshjou et al. - SkinCon A skin disease dataset densely annotated .pdf}
}

@incollection{danksGovernanceExplainability2022,
  title = {Governance via {{Explainability}}},
  booktitle = {The {{Oxford Handbook}} of {{AI Governance}}},
  author = {Danks, David},
  editor = {Bullock, Justin B. and Chen, Yu-Che and Himmelreich, Johannes and Hudson, Valerie M. and Korinek, Anton and Young, Matthew M. and Zhang, Baobao},
  date = {2022-02-14},
  edition = {1},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780197579329.013.11},
  url = {https://academic.oup.com/edited-volume/41989/chapter/355437387},
  urldate = {2023-10-09},
  abstract = {AI governance often requires knowing why the system behaved as it did, and explanations are a common way to convey this kind of why-information. Explainable AI (XAI) thus seems to be particularly well-suited to governance; one might even think that explainability is a prerequisite for AI governance. This chapter explores this intuitively plausible route of AI governance via explainability. The core challenge is that governance, explanations, and XAI are all signi cantly more complex than this intuitive connection suggests, creating the risk that the explanations provided by XAI are not the kind required for governance. This chapter thus rst provides a high-level overview of three types of XAI that di er based on who generates the explanation (AI vs. human) and the grounding of the explanation (facts about system vs. plausibility of the story). These di erent types of XAI each presuppose a substantive theory of explanations, so the chapter then provides an overview of both philosophical and psychological theories of explanation. Finally, these pieces are brought together to provide a concrete framework for using XAI to create, support, or enable many of the key functions of AI governance. XAI systems are not necessarily more governable than non-XAI systems, nor is explainability a solution for all challenges of AI governance. However, explainability does provide a valuable tool in the design and implementation of AI governance mechanisms.},
  isbn = {978-0-19-757932-9 978-0-19-757935-0},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\A5QRZPKK\Danks - 2022 - Governance via Explainability.pdf}
}

@online{doranWhatDoesExplainable2017,
  title = {What {{Does Explainable AI Really Mean}}? {{A New Conceptualization}} of {{Perspectives}}},
  shorttitle = {What {{Does Explainable AI Really Mean}}?},
  author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R.},
  date = {2017-10-02},
  eprint = {1710.00794},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1710.00794},
  urldate = {2023-08-06},
  abstract = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algorithmic mechanisms; interpretable systems where users can mathematically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C:\Users\c21012241\Zotero\storage\E4RH69L9\Doran et al. - 2017 - What Does Explainable AI Really Mean A New Concep.pdf}
}

@online{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {Doshi-Velez, Finale and Kim, Been},
  date = {2017-03-02},
  eprint = {1702.08608},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1702.08608},
  urldate = {2023-08-24},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\S78ZA6VH\Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf}
}

@online{ElevenLabsGenerativeAI,
  title = {{{ElevenLabs}} - {{Generative AI Text}} to {{Speech}} \& {{Voice Cloning}}},
  url = {https://elevenlabs.io/speech-synthesis},
  urldate = {2023-09-12},
  file = {C:\Users\c21012241\Zotero\storage\YYR228MY\speech-synthesis.html}
}

@incollection{eltonSelfexplainingAIAlternative2020,
  title = {Self-Explaining {{AI}} as an {{Alternative}} to {{Interpretable AI}}},
  booktitle = {Artificial {{General Intelligence}}},
  author = {Elton, Daniel C.},
  editor = {Goertzel, Ben and Panov, Aleksandr I. and Potapov, Alexey and Yampolskiy, Roman},
  date = {2020},
  volume = {12177},
  pages = {95--106},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-52152-3_10},
  url = {http://link.springer.com/10.1007/978-3-030-52152-3_10},
  urldate = {2023-10-10},
  isbn = {978-3-030-52151-6 978-3-030-52152-3},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\UWE5YWPD\Self-explaining AI as an alternative to interpretable AI - Elton - 2020.pdf}
}

@article{erdfelderGPOWERGeneralPower1996,
  title = {{{GPOWER}}: {{A}} General Power Analysis Program},
  shorttitle = {{{GPOWER}}},
  author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  date = {1996-03},
  journaltitle = {Behavior Research Methods, Instruments, \& Computers},
  shortjournal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {28},
  number = {1},
  pages = {1--11},
  issn = {0743-3808, 1532-5970},
  doi = {10.3758/BF03203630},
  url = {http://link.springer.com/10.3758/BF03203630},
  urldate = {2023-10-09},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\BJE38A7I\Erdfelder et al. - 1996 - GPOWER A general power analysis program.pdf}
}

@article{ericksonRulesExemplarsCategory1996,
  title = {Rules and {{Exemplars}} in {{Category Learning}}},
  author = {Erickson, Michael A and Kruschke, John K},
  date = {1996},
  journaltitle = {Journal of Experimental Psychology General},
  volume = {127},
  doi = {10.1037/0096-3445.127.2.107},
  url = {https://www.researchgate.net/publication/13661419_Rules_and_Exemplars_in_Category_Learning},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\JRXW6ISE\Erickson and Kruschke - Rules and Exemplars in Category Learning.pdf}
}

@book{europeanunionintellectualpropertyoffice.StudyImpactArtificial2022,
  title = {Study on the Impact of Artificial Intelligence on the Infringement and Enforcement of Copyright and Designs.},
  author = {{European Union Intellectual Property Office.}},
  date = {2022},
  publisher = {{Publications Office}},
  location = {{LU}},
  url = {https://data.europa.eu/doi/10.2814/062663},
  urldate = {2023-08-25},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\WKMUQLXF\European Union Intellectual Property Office. - 2022 - Study on the impact of artificial intelligence on .pdf}
}

@online{evansClimateCrisisPaintings2022,
  title = {Climate Crisis Paintings by Famous Artists Using {{AI}}},
  author = {Evans, Gareth},
  date = {2022-12-01T17:36:13+00:00},
  url = {https://www.artsupplies.co.uk/blog/climate-crisis-paintings-by-famous-artists-using-ai/},
  urldate = {2023-08-29},
  abstract = {We decided to use the medium of art to raise awareness of the climate emergency. We used AI to show how famous artists from history might depict the climate crisis if they were living today.},
  langid = {british},
  organization = {{Ken Bromley Art Supplies}},
  file = {C:\Users\c21012241\Zotero\storage\MNRGYJ5V\climate-crisis-paintings-by-famous-artists-using-ai.html}
}

@online{federaltradecommissionconsumeradviceScammersUseAI2023,
  title = {Scammers Use {{AI}} to Enhance Their Family Emergency Schemes},
  author = {Federal Trade Commission Consumer Advice},
  date = {2023-03-17T14:48:09-04:00},
  url = {https://consumer.ftc.gov/consumer-alerts/2023/03/scammers-use-ai-enhance-their-family-emergency-schemes},
  urldate = {2023-08-29},
  abstract = {You get a call. There's a panicked voice on the line. It's your grandson. He says he's in deep trouble — he wrecked the car and landed in jail. But you can help by sending money. You take a deep breath and think. You've heard about grandparent scams. But darn, it sounds just like him. How could it be a scam? Voice cloning, that's how.},
  langid = {english},
  organization = {{Consumer Advice}},
  file = {C:\Users\c21012241\Zotero\storage\LFCYTWB3\scammers-use-ai-enhance-their-family-emergency-schemes.html}
}

@article{flemingLearningSeeStuff2019,
  title = {Learning to See Stuff},
  author = {Fleming, Roland W and Storrs, Katherine R},
  date = {2019-12},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  volume = {30},
  pages = {100--108},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2019.07.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2352154619300397},
  urldate = {2023-08-06},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\GSH6VQZQ\Fleming and Storrs - 2019 - Learning to see stuff.pdf}
}

@article{flemingMaterialPerception2017,
  title = {Material {{Perception}}},
  author = {Fleming, Roland W},
  date = {2017},
  abstract = {Under typical viewing conditions, human observers effortlessly recognize materials and infer their physical, functional, and multisensory properties at a glance. Without touching materials, we can usually tell whether they would feel hard or soft, rough or smooth, wet or dry. We have vivid visual intuitions about how deformable materials like liquids or textiles respond to external forces and how surfaces like chrome, wax, or leather change appearance when formed into different shapes or viewed under different lighting. These achievements are impressive because the retinal image results from complex optical interactions between lighting, shape, and material, which cannot easily be disentangled. Here I argue that because of the diversity, mutability, and complexity of materials, they pose enormous challenges to vision science: What is material appearance, and how do we measure it? How are material properties estimated and represented? Resolving these questions causes us to scrutinize the basic assumptions of mid-level vision.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\5YLSND74\Fleming - 2017 - Material Perception.pdf}
}

@inproceedings{fongInterpretableExplanationsBlack2017,
  title = {Interpretable {{Explanations}} of {{Black Boxes}} by {{Meaningful Perturbation}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Fong, Ruth C. and Vedaldi, Andrea},
  date = {2017-10},
  pages = {3449--3457},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.371},
  url = {http://ieeexplore.ieee.org/document/8237633/},
  urldate = {2023-10-10},
  abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks “look” in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\PKQ8PLRN\Fong and Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meani.pdf}
}

@article{ghorbaniNeuronShapleyDiscovering2020,
  title = {Neuron {{Shapley}}: {{Discovering}} the {{Responsible Neurons}}},
  author = {Ghorbani, Amirata and Zou, James},
  date = {2020},
  abstract = {We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Computing exact Shapley values is computationally infeasible and therefore sampling-based approximations are used in practice. We introduce a new multi-armed bandit algorithm that is able to efficiently detect neurons with the largest Shapley value orders of magnitude faster than existing Shapley value approximation methods.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\7UHGWLUD\Ghorbani and Zou - Neuron Shapley Discovering the Responsible Neuron.pdf}
}

@article{gilsonHowDoesChatGPT2023,
  title = {How {{Does ChatGPT Perform}} on the {{United States Medical Licensing Examination}}? {{The Implications}} of {{Large Language Models}} for {{Medical Education}} and {{Knowledge Assessment}}},
  shorttitle = {How {{Does ChatGPT Perform}} on the {{United States Medical Licensing Examination}}?},
  author = {Gilson, Aidan and Safranek, Conrad W and Huang, Thomas and Socrates, Vimig and Chi, Ling and Taylor, Richard Andrew and Chartash, David},
  date = {2023-02-08},
  journaltitle = {JMIR Medical Education},
  shortjournal = {JMIR Med Educ},
  volume = {9},
  pages = {e45312},
  issn = {2369-3762},
  doi = {10.2196/45312},
  url = {https://mededu.jmir.org/2023/1/e45312},
  urldate = {2023-10-07},
  abstract = {Background: Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective: This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods: We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\JEEQX2WP\Gilson et al. - 2023 - How Does ChatGPT Perform on the United States Medi.PDF}
}

@article{goodmanEuropeanUnionRegulations2017a,
  title = {European {{Union}} Regulations on Algorithmic Decision-Making and a "Right to Explanation"},
  author = {Goodman, Bryce and Flaxman, Seth},
  date = {2017-09},
  journaltitle = {AI Magazine},
  shortjournal = {AI Magazine},
  volume = {38},
  number = {3},
  eprint = {1606.08813},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {50--57},
  issn = {0738-4602, 2371-9621},
  doi = {10.1609/aimag.v38i3.2741},
  url = {http://arxiv.org/abs/1606.08813},
  urldate = {2023-08-24},
  abstract = {We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on userlevel predictors) which “significantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\HZKDDANH\Goodman and Flaxman - 2017 - European Union regulations on algorithmic decision.pdf}
}

@online{googleKlimtVsKlimt2021,
  title = {Klimt vs. {{Klimt}} — {{Google Arts}} \& {{Culture}}},
  author = {Google},
  date = {2021},
  url = {https://artsandculture.google.com/project/klimt-vs-klimt},
  urldate = {2023-08-29},
  file = {C:\Users\c21012241\Zotero\storage\XDTAXXLQ\klimt-vs-klimt.html}
}

@article{gorissenSurrogateModelingAdaptive2010,
  title = {A {{Surrogate Modeling}} and {{Adaptive Sampling Toolbox}} for {{Computer Based Design}}},
  author = {Gorissen, Dirk and Couckuyt, Ivo and Demeester, Piet and Dhaene, Tom and Crombecq, Karel},
  date = {2010},
  abstract = {An exceedingly large number of scientific and engineering fields are confronted with the need for computer simulations to study complex, real world phenomena or solve challenging design problems. However, due to the computational cost of these high fidelity simulations, the use of neural networks, kernel methods, and other surrogate modeling techniques have become indispensable. Surrogate models are compact and cheap to evaluate, and have proven very useful for tasks such as optimization, design space exploration, prototyping, and sensitivity analysis. Consequently, in many fields there is great interest in tools and techniques that facilitate the construction of such regression models, while minimizing the computational cost and maximizing model accuracy. This paper presents a mature, flexible, and adaptive machine learning toolkit for regression modeling and active learning to tackle these issues. The toolkit brings together algorithms for data fitting, model selection, sample selection (active learning), hyperparameter optimization, and distributed computing in order to empower a domain expert to efficiently generate an accurate model for the problem or data at hand.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\DDMMBFTT\Gorissen et al. - A Surrogate Modeling and Adaptive Sampling Toolbox.pdf}
}

@article{grangeXAISelfexplanatoryAI2022,
  title = {{{XAI}} \& {{I}}: Self-Explanatory {{AI}} Facilitating Mutual Understanding between {{AI}} and Human Experts},
  shorttitle = {{{XAI}} \& {{I}}},
  author = {Grange, Jacques A. and Princis, Henrijs and Kozlowski, Theodor R.W. and Amadou-Dioffo, Aissa and Wu, Jing and Hicks, Yulia A. and Johansen, Mark K.},
  date = {2022},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {207},
  pages = {3600--3607},
  issn = {18770509},
  doi = {10.1016/j.procs.2022.09.419},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050922013114},
  urldate = {2023-07-11},
  abstract = {Traditionally, explainable artificial intelligence seeks to provide explanation and interpretability of high-performing black-box models such as deep neural networks. Interpretation of such models remains difficult, because of their high complexity. An alternative method is to instead force a deep-neural network to use human-intelligible features as the basis for its decisions. We tested this approach using the natural category domain of rock types. We compared the performance of a black-box implementation of transfer-learning using Resnet50 to that of a network first trained to predict expert-identified features and then forced to use these features to categorise rock images. The performance of this feature-constrained network was virtually identical to that of the unconstrained network. Further, a partially constrained network forced to condense down to a small number of features that was not trained with expert features did not result in these abstracted features being intelligible; nevertheless, an affine transformation of these features could be found that aligned well with expert-intelligible features. These findings show that making an AI intrinsically intelligible need not be at the cost of performance.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\XMV498TA\Grange et al. - 2022 - XAI & I Self-explanatory AI facilitating mutual u.pdf}
}

@incollection{greeneAIGovernanceMultistakeholder2022,
  title = {{{AI Governance Multi-stakeholder Convening}}},
  booktitle = {The {{Oxford Handbook}} of {{AI Governance}}},
  author = {Greene, K. Gretchen},
  editor = {Bullock, Justin B. and Chen, Yu-Che and Himmelreich, Johannes and Hudson, Valerie M. and Korinek, Anton and Young, Matthew M. and Zhang, Baobao},
  date = {2022-03-18},
  edition = {1},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780197579329.013.6},
  url = {https://academic.oup.com/edited-volume/41989/chapter/355436921},
  urldate = {2023-10-09},
  abstract = {This chapter o ers re ections and advice on AI ethics and governance from a year spent leading Partnership on AI’s multi-stakeholder A ective Computing and Ethics project, involving more than 200 engineers, scientists, lawyers, privacy and civil rights advocates, bioethicists, managers, executives, journalists, and government o cials, mostly in the United States, United Kingdom, and European Union, in discussions about AI related to emotion and a ect and its potential impacts on civil and human rights, with a goal of developing industry best practices and better technology policy. The author’s re ections from that year draw lessons on convening, multi-disciplinary collaboration, and a ective computing and AI ethics and governance. The chapter o ers a blueprint for creating the shared knowledge foundation non-technical participants need to apply their expertise. It presents question exploration as a tool for evaluating ethics risk and using “What is notice good for?” shows how a well-chosen question can serve as a catalyst for group or individual exploration of the issues, leading to insights and answers. It includes a curated list of 42 questions, catalysts for AI and ethics discussion for industry, university, news media, and policy teams; multi-disciplinary, multistakeholder AI ethics and governance convenings; and for individual writing and thinking, to take scholars and practitioners a step beyond or to the side of what they have been thinking about—about AI and ethics.},
  isbn = {978-0-19-757932-9 978-0-19-757935-0},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\LU7BVE6B\Greene - 2022 - AI Governance Multi-stakeholder Convening.pdf}
}

@article{grossGenealogyGrandmotherCell2002,
  title = {Genealogy of the “{{Grandmother Cell}}”},
  author = {Gross, Charles G.},
  date = {2002-10},
  journaltitle = {The Neuroscientist},
  shortjournal = {Neuroscientist},
  volume = {8},
  number = {5},
  pages = {512--518},
  issn = {1073-8584, 1089-4098},
  doi = {10.1177/107385802237175},
  url = {http://journals.sagepub.com/doi/10.1177/107385802237175},
  urldate = {2023-08-29},
  abstract = {A “grandmother cell” is a hypothetical neuron that responds only to a highly complex, specific, and meaningful stimulus, such as the image of one’s grandmother. The term originated in a parable Jerry Lettvin told in 1967. A similar concept had been systematically developed a few years earlier by Jerzy Konorski who called such cells “gnostic” units. This essay discusses the origin, influence, and current status of these terms and of the alternative view that complex stimuli are represented by the pattern of firing across ensembles of neurons.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\9TRHS9CN\Gross - 2002 - Genealogy of the “Grandmother Cell”.pdf}
}

@article{guidottiSurveyMethodsExplaining2019,
  title = {A {{Survey}} of {{Methods}} for {{Explaining Black Box Models}}},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  date = {2019-09-30},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {51},
  number = {5},
  pages = {1--42},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3236009},
  url = {https://dl.acm.org/doi/10.1145/3236009},
  urldate = {2023-08-29},
  abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\3XJEKLIN\Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf}
}

@article{gunningDARPAExplainableArtificial2019,
  title = {{{DARPA}}'s {{Explainable Artificial Intelligence Program}}},
  author = {Gunning, David and Aha, David W.},
  date = {2019-06},
  journaltitle = {AI Magazine},
  shortjournal = {AI Magazine},
  volume = {40},
  number = {2},
  pages = {44--58},
  issn = {0738-4602, 2371-9621},
  doi = {10.1609/aimag.v40i2.2850},
  url = {https://onlinelibrary.wiley.com/doi/10.1609/aimag.v40i2.2850},
  urldate = {2023-08-15},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\29MKSSEX\Gunning and Aha - 2019 - DARPA's Explainable Artificial Intelligence Progra.pdf}
}

@article{gunningExplainableArtificialIntelligence2017,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Gunning, David},
  date = {2017},
  url = {https://sites.cc.gatech.edu/~alanwags/DLAI2016/(Gunning)%20IJCAI-16%20DLAI%20WS.pdf},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\LSNPHHEL\Gunning - Explainable Artificial Intelligence (XAI).pdf}
}

@article{guoSurveyAutomatedFactChecking2022,
  title = {A {{Survey}} on {{Automated Fact-Checking}}},
  author = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  date = {2022-02-09},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {178--206},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00454},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00454/109469/A-Survey-on-Automated-Fact-Checking},
  urldate = {2023-08-28},
  abstract = {Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how factchecking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\5KD8FW5R\Guo et al. - 2022 - A Survey on Automated Fact-Checking.pdf}
}

@article{hassijaInterpretingBlackBoxModels2023,
  title = {Interpreting {{Black-Box Models}}: {{A Review}} on {{Explainable Artificial Intelligence}}},
  shorttitle = {Interpreting {{Black-Box Models}}},
  author = {Hassija, Vikas and Chamola, Vinay and Mahapatra, Atmesh and Singal, Abhinandan and Goel, Divyansh and Huang, Kaizhu and Scardapane, Simone and Spinelli, Indro and Mahmud, Mufti and Hussain, Amir},
  date = {2023-08-24},
  journaltitle = {Cognitive Computation},
  shortjournal = {Cogn Comput},
  issn = {1866-9956, 1866-9964},
  doi = {10.1007/s12559-023-10179-8},
  url = {https://link.springer.com/10.1007/s12559-023-10179-8},
  urldate = {2023-10-06},
  abstract = {Recent years have seen a tremendous growth in Artificial Intelligence (AI)-based methodological development in a broad range of domains. In this rapidly evolving field, large number of methods are being reported using machine learning (ML) and Deep Learning (DL) models. Majority of these models are inherently complex and lacks explanations of the decision making process causing these models to be termed as 'Black-Box'. One of the major bottlenecks to adopt such models in mission-critical application domains, such as banking, e-commerce, healthcare, and public services and safety, is the difficulty in interpreting them. Due to the rapid proleferation of these AI models, explaining their learning and decision making process are getting harder which require transparency and easy predictability. Aiming to collate the current state-of-the-art in interpreting the black-box models, this study provides a comprehensive analysis of the explainable AI (XAI) models. To reduce false negative and false positive outcomes of these back-box models, finding flaws in them is still difficult and inefficient. In this paper, the development of XAI is reviewed meticulously through careful selection and analysis of the current state-of-the-art of XAI research. It also provides a comprehensive and in-depth evaluation of the XAI frameworks and their efficacy to serve as a starting point of XAI for applied and theoretical researchers. Towards the end, it highlights emerging and critical issues pertaining to XAI research to showcase major, model-specific trends for better explanation, enhanced transparency, and improved prediction accuracy.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\8THJJ767\Hassija et al. - 2023 - Interpreting Black-Box Models A Review on Explain.pdf}
}

@article{havasiAddressingLeakageConcept2022a,
  title = {Addressing {{Leakage}} in {{Concept Bottleneck Models}}},
  author = {Havasi, Marton and Parbhoo, Sonali and Doshi-Velez, Finale},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {23386--23397},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/944ecf65a46feb578a43abfd5cddd960-Abstract-Conference.html},
  urldate = {2023-10-12},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\JS9WVKP7\Havasi et al. - 2022 - Addressing Leakage in Concept Bottleneck Models.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.90},
  url = {http://ieeexplore.ieee.org/document/7780459/},
  urldate = {2023-07-11},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\35HCPFUC\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{hindTEDTeachingAI2019,
  title = {{{TED}}: {{Teaching AI}} to {{Explain}} Its {{Decisions}}},
  shorttitle = {{{TED}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Hind, Michael and Wei, Dennis and Campbell, Murray and Codella, Noel C. F. and Dhurandhar, Amit and Mojsilović, Aleksandra and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R.},
  date = {2019-01-27},
  pages = {123--129},
  publisher = {{ACM}},
  location = {{Honolulu HI USA}},
  doi = {10.1145/3306618.3314273},
  url = {https://dl.acm.org/doi/10.1145/3306618.3314273},
  urldate = {2023-08-27},
  abstract = {Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.},
  eventtitle = {{{AIES}} '19: {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  isbn = {978-1-4503-6324-2},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\9ZJP9VP4\Hind et al. - 2019 - TED Teaching AI to Explain its Decisions.pdf}
}

@article{hoffmanEvaluatingMachinegeneratedExplanations2023,
  title = {Evaluating Machine-Generated Explanations: A “{{Scorecard}}” Method for {{XAI}} Measurement Science},
  shorttitle = {Evaluating Machine-Generated Explanations},
  author = {Hoffman, Robert R. and Jalaeian, Mohammadreza and Tate, Connor and Klein, Gary and Mueller, Shane T.},
  date = {2023-05-09},
  journaltitle = {Frontiers in Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {5},
  pages = {1114806},
  issn = {2624-9898},
  doi = {10.3389/fcomp.2023.1114806},
  url = {https://www.frontiersin.org/articles/10.3389/fcomp.2023.1114806/full},
  urldate = {2023-10-07},
  abstract = {Introduction                                Many Explainable AI (XAI) systems provide explanations that are just clues or hints about the computational models-Such things as feature lists, decision trees, or saliency images. However, a user might want answers to deeper questions such as                 How does it work?, Why did it do that instead of something else? What things can it get wrong?                 How might XAI system developers evaluate existing XAI systems with regard to the depth of support they provide for the user's sensemaking? How might XAI system developers shape new XAI systems so as to support the user's sensemaking? What might be a useful conceptual terminology to assist developers in approaching this challenge?                                                        Method               Based on cognitive theory, a scale was developed reflecting depth of explanation, that is, the degree to which explanations support the user's sensemaking. The seven levels of this scale form the Explanation Scorecard.                                         Results and discussion               The Scorecard was utilized in an analysis of recent literature, showing that many systems still present low-level explanations. The Scorecard can be used by developers to conceptualize how they might extend their machine-generated explanations to support the user in developing a mental model that instills appropriate trust and reliance. The article concludes with recommendations for how XAI systems can be improved with regard to the cognitive considerations, and recommendations regarding the manner in which results on the evaluation of XAI systems are reported.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\DDFQSQEE\Hoffman et al. - 2023 - Evaluating machine-generated explanations a “Scor.pdf}
}

@article{hoffmanExplainableAIRoles2023,
  title = {Explainable {{AI}}: Roles and Stakeholders, Desirements and Challenges},
  shorttitle = {Explainable {{AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Jalaeian, Mohammadreza and Tate, Connor},
  date = {2023-08-17},
  journaltitle = {Frontiers in Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {5},
  pages = {1117848},
  issn = {2624-9898},
  doi = {10.3389/fcomp.2023.1117848},
  url = {https://www.frontiersin.org/articles/10.3389/fcomp.2023.1117848/full},
  urldate = {2023-08-25},
  abstract = {Introduction               The purpose of the Stakeholder Playbook is to enable the developers of explainable AI systems to take into account the different ways in which different stakeholders or role-holders need to “look inside” the AI/XAI systems.                                         Method               We conducted structured cognitive interviews with senior and mid-career professionals who had direct experience either developing or using AI and/or autonomous systems.                                         Results               The results show that role-holders need access to others (e.g., trusted engineers and trusted vendors) for them to be able to develop satisfying mental models of AI systems. They need to know how it fails and misleads as much as they need to know how it works. Some stakeholders need to develop an understanding that enables them to explain the AI to someone else and not just satisfy their own sense-making requirements. Only about half of our interviewees said they always wanted explanations or even needed better explanations than the ones that were provided. Based on our empirical evidence, we created a “Playbook” that lists explanation desires, explanation challenges, and explanation cautions for a variety of stakeholder groups and roles.                                         Discussion               This and other findings seem surprising, if not paradoxical, but they can be resolved by acknowledging that different role-holders have differing skill sets and have different sense-making desires. Individuals often serve in multiple roles and, therefore, can have different immediate goals. The goal of the Playbook is to help XAI developers by guiding the development process and creating explanations that support the different roles.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\3PWLN37J\Hoffman et al. - 2023 - Explainable AI roles and stakeholders, desirement.pdf}
}

@article{hoffmanExplainingExplanationPart2017,
  title = {Explaining {{Explanation}}, {{Part}} 1: {{Theoretical Foundations}}},
  shorttitle = {Explaining {{Explanation}}, {{Part}} 1},
  author = {Hoffman, Robert R. and Klein, Gary},
  date = {2017-05},
  journaltitle = {IEEE Intelligent Systems},
  shortjournal = {IEEE Intell. Syst.},
  volume = {32},
  number = {3},
  pages = {68--73},
  issn = {1541-1672},
  doi = {10.1109/MIS.2017.54},
  url = {http://ieeexplore.ieee.org/document/7933919/},
  urldate = {2023-08-14},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\JAHBLM8D\Hoffman and Klein - 2017 - Explaining Explanation, Part 1 Theoretical Founda.pdf}
}

@article{hoffmanExplainingExplanationPart2017a,
  title = {Explaining {{Explanation}}, {{Part}} 2: {{Empirical Foundations}}},
  shorttitle = {Explaining {{Explanation}}, {{Part}} 2},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary},
  date = {2017},
  journaltitle = {IEEE Intelligent Systems},
  shortjournal = {IEEE Intell. Syst.},
  volume = {32},
  number = {4},
  pages = {78--86},
  issn = {1541-1672},
  doi = {10.1109/MIS.2017.3121544},
  url = {http://ieeexplore.ieee.org/document/8012316/},
  urldate = {2023-08-14},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\I4K4GC7F\Hoffman et al. - 2017 - Explaining Explanation, Part 2 Empirical Foundati.pdf}
}

@article{hoffmanExplainingExplanationPart2018,
  title = {Explaining {{Explanation}}, {{Part}} 4: {{A Deep Dive}} on {{Deep Nets}}},
  shorttitle = {Explaining {{Explanation}}, {{Part}} 4},
  author = {Hoffman, Robert and Miller, Tim and Mueller, Shane T. and Klein, Gary and Clancey, William J.},
  date = {2018-05},
  journaltitle = {IEEE Intelligent Systems},
  shortjournal = {IEEE Intell. Syst.},
  volume = {33},
  number = {3},
  pages = {87--95},
  issn = {1541-1672, 1941-1294},
  doi = {10.1109/MIS.2018.033001421},
  url = {https://ieeexplore.ieee.org/document/8423529/},
  urldate = {2023-08-14},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\EK3EVNEE\Hoffman et al. - 2018 - Explaining Explanation, Part 4 A Deep Dive on Dee.pdf}
}

@article{hoffmanIncreasingValueXAI2023,
  title = {Increasing the {{Value}} of {{XAI}} for {{Users}}: {{A Psychological Perspective}}},
  shorttitle = {Increasing the {{Value}} of {{XAI}} for {{Users}}},
  author = {Hoffman, Robert R. and Miller, Timothy and Klein, Gary and Mueller, Shane T. and Clancey, William J.},
  date = {2023-07-17},
  journaltitle = {KI - Künstliche Intelligenz},
  shortjournal = {Künstl Intell},
  issn = {0933-1875, 1610-1987},
  doi = {10.1007/s13218-023-00806-9},
  url = {https://link.springer.com/10.1007/s13218-023-00806-9},
  urldate = {2023-10-09},
  abstract = {This paper summarizes the psychological insights and related design challenges that have emerged in the field of Explainable AI (XAI). This summary is organized as a set of principles, some of which have recently been instantiated in XAI research. The primary aspects of implementation to which the principles refer are the design and evaluation stages of XAI system development, that is, principles concerning the design of explanations and the design of experiments for evaluating the performance of XAI systems. The principles can serve as guidance, to ensure that AI systems are human-centered and effectively assist people in solving difficult problems.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\N3ZZC85V\Hoffman et al. - 2023 - Increasing the Value of XAI for Users A Psycholog.pdf}
}

@article{hoffmanMeasuresExplainableAI2023,
  title = {Measures for Explainable {{AI}}: {{Explanation}} Goodness, User Satisfaction, Mental Models, Curiosity, Trust, and Human-{{AI}} Performance},
  shorttitle = {Measures for Explainable {{AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  date = {2023-02-06},
  journaltitle = {Frontiers in Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {5},
  pages = {1096257},
  issn = {2624-9898},
  doi = {10.3389/fcomp.2023.1096257},
  url = {https://www.frontiersin.org/articles/10.3389/fcomp.2023.1096257/full},
  urldate = {2023-08-06},
  abstract = {If a user is presented an AI system that portends to explain how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? This question entails some key concepts of measurement such as explanation goodness and trust. We present methods for enabling developers and researchers to: (1) Assess the               a priori               goodness of explanations, (2) Assess users' satisfaction with explanations, (3) Reveal user's mental model of an AI system, (4) Assess user's curiosity or need for explanations, (5) Assess whether the user's trust and reliance on the AI are appropriate, and finally, (6) Assess how the human-XAI work system performs. The methods we present derive from our integration of extensive research literatures and our own psychometric evaluations. We point to the previous research that led to the measurement scales which we aggregated and tailored specifically for the XAI context. Scales are presented in sufficient detail to enable their use by XAI researchers. For Mental Model assessment and Work System Performance, XAI researchers have choices. We point to a number of methods, expressed in terms of methods' strengths and weaknesses, and pertinent measurement issues.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\4AWYB8PJ\Hoffman et al. - 2023 - Measures for explainable AI Explanation goodness,.pdf}
}

@book{holzingerXxAIExplainableAI2022a,
  title = {{{xxAI}} - {{Beyond Explainable AI}}: {{International Workshop}}, {{Held}} in {{Conjunction}} with {{ICML}} 2020, {{July}} 18, 2020, {{Vienna}}, {{Austria}}, {{Revised}} and {{Extended Papers}}},
  shorttitle = {{{xxAI}} - {{Beyond Explainable AI}}},
  editor = {Holzinger, Andreas and Goebel, Randy and Fong, Ruth and Moon, Taesup and Müller, Klaus-Robert and Samek, Wojciech},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13200},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-04083-2},
  url = {https://link.springer.com/10.1007/978-3-031-04083-2},
  urldate = {2023-10-06},
  isbn = {978-3-031-04082-5 978-3-031-04083-2},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\86RP9VPD\Holzinger et al. - 2022 - xxAI - Beyond Explainable AI International Worksh.pdf}
}

@article{hookerBenchmarkInterpretabilityMethods2019,
  title = {A {{Benchmark}} for {{Interpretability Methods}} in {{Deep Neural Networks}}},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  date = {2019},
  abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches—VarGrad and SmoothGrad-Squared—outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\VBD64ESK\Hooker et al. - A Benchmark for Interpretability Methods in Deep N.pdf}
}

@inproceedings{jesusHowCanChoose2021,
  title = {How Can {{I}} Choose an Explainer?: {{An Application-grounded Evaluation}} of {{Post-hoc Explanations}}},
  shorttitle = {How Can {{I}} Choose an Explainer?},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Jesus, Sérgio and Belém, Catarina and Balayan, Vladimir and Bento, João and Saleiro, Pedro and Bizarro, Pedro and Gama, João},
  date = {2021-03-03},
  pages = {805--815},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  doi = {10.1145/3442188.3445941},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445941},
  urldate = {2023-10-07},
  abstract = {There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular XAI methods – LIME, SHAP, and TreeInterpreter – on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to case.},
  eventtitle = {{{FAccT}} '21: 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\9H7PD8AK\Jesus et al. - 2021 - How can I choose an explainer An Application-gro.pdf}
}

@article{jianFoundationsEmpiricallyDetermined2000,
  title = {Foundations for an {{Empirically Determined Scale}} of {{Trust}} in {{Automated Systems}}},
  author = {Jian, Jiun-Yin and Bisantz, Ann M. and Drury, Colin G.},
  date = {2000-03-01},
  journaltitle = {International Journal of Cognitive Ergonomics},
  shortjournal = {International Journal of Cognitive Ergonomics},
  volume = {4},
  number = {1},
  pages = {53--71},
  publisher = {{Routledge}},
  issn = {1088-6362},
  doi = {10.1207/S15327566IJCE0401_04},
  url = {https://doi.org/10.1207/S15327566IJCE0401_04}
}

@article{khosraviExplainableArtificialIntelligence2022,
  title = {Explainable {{Artificial Intelligence}} in Education},
  author = {Khosravi, Hassan and Shum, Simon Buckingham and Chen, Guanliang and Conati, Cristina and Tsai, Yi-Shan and Kay, Judy and Knight, Simon and Martinez-Maldonado, Roberto and Sadiq, Shazia and Gašević, Dragan},
  date = {2022},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100074},
  issn = {2666920X},
  doi = {10.1016/j.caeai.2022.100074},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666920X22000297},
  urldate = {2023-09-12},
  abstract = {There are emerging concerns about the Fairness, Accountability, Transparency, and Ethics (FATE) of educational interventions supported by the use of Artificial Intelligence (AI) algorithms. One of the emerging methods for increasing trust in AI systems is to use eXplainable AI (XAI), which promotes the use of methods that produce transparent explanations and reasons for decisions AI systems make. Considering the existing literature on XAI, this paper argues that XAI in education has commonalities with the broader use of AI but also has distinctive needs. Accordingly, we first present a framework, referred to as XAI-ED, that considers six key aspects in relation to explainability for studying, designing and developing educational AI tools. These key aspects focus on the stakeholders, benefits, approaches for presenting explanations, widely used classes of AI models, human-centred designs of the AI interfaces and potential pitfalls of providing explanations within education. We then present four comprehensive case studies that illustrate the application of XAI-ED in four different educational AI tools. The paper concludes by discussing opportunities, challenges and future research needs for the effective incor­ poration of XAI in education.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\MWGTUUIX\Khosravi et al. - 2022 - Explainable Artificial Intelligence in education.pdf}
}

@inproceedings{kimInterpretabilityFeatureAttribution2018,
  title = {Interpretability {{Beyond Feature Attribution}}:  {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/kim18d.html},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\Y5MAJNGN\Kim et al. - Interpretability Beyond Feature Attribution  Quan.pdf}
}

@article{kleinExplainingExplanationPart2018,
  title = {Explaining {{Explanation}}, {{Part}} 3: {{The Causal Landscape}}},
  shorttitle = {Explaining {{Explanation}}, {{Part}} 3},
  author = {Klein, Gary},
  date = {2018-03},
  journaltitle = {IEEE Intelligent Systems},
  shortjournal = {IEEE Intell. Syst.},
  volume = {33},
  number = {2},
  pages = {83--88},
  issn = {1541-1672, 1941-1294},
  doi = {10.1109/MIS.2018.022441353},
  url = {https://ieeexplore.ieee.org/document/8378482/},
  urldate = {2023-08-14},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\7QB46YCD\Klein - 2018 - Explaining Explanation, Part 3 The Causal Landsca.pdf}
}

@article{kleinMinimumNecessaryRigor2023,
  title = {“{{Minimum Necessary Rigor}}” in Empirically Evaluating Human–{{AI}} Work Systems},
  author = {Klein, Gary and Hoffman, Robert R. and Clancey, William J. and Mueller, Shane T. and Jentsch, Florian and Jalaeian, Mohammadreza},
  date = {2023-09},
  journaltitle = {AI Magazine},
  shortjournal = {AI Magazine},
  volume = {44},
  number = {3},
  pages = {274--281},
  issn = {0738-4602, 2371-9621},
  doi = {10.1002/aaai.12108},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/aaai.12108},
  urldate = {2023-10-09},
  abstract = {The development of AI systems represents a significant investment of funds and time. Assessment is necessary in order to determine whether that investment has paid off. Empirical evaluation of systems in which humans and AI systems act interdependently to accomplish tasks must provide convincing empirical evidence that the work system is learnable and that the technology is usable and useful. We argue that the assessment of human–AI (HAI) systems must be effective but must also be efficient. Bench testing of a prototype of an HAI system cannot require extensive series of large-scale experiments with complex designs. Some of the constraints that are imposed in traditional laboratory research just are not appropriate for the empirical evaluation of HAI systems. We present requirements for avoiding “unnecessary rigor.” They cover study design, research methods, statistical analyses, and online experimentation. These should be applicable to all research intended to evaluate the effectiveness of HAI systems.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\VZSY6WSA\Klein et al. - 2023 - “Minimum Necessary Rigor” in empirically evaluatin.pdf}
}

@online{kohConceptBottleneckModels2020,
  title = {Concept {{Bottleneck Models}}},
  author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  date = {2020-12-28},
  eprint = {2007.04612},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.04612},
  urldate = {2023-08-15},
  abstract = {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like “the existence of bone spurs”, as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts (“bone spurs”) or bird attributes (“wing color”). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\DJLEL63Q\Koh et al. - 2020 - Concept Bottleneck Models.pdf}
}

@article{kruschkeALCOVEExemplarBasedConnectionist1992,
  title = {{{ALCOVE}}: {{An Exemplar-Based Connectionist Model}} of {{Category Learning}}},
  shorttitle = {{{ALCOVE}}},
  author = {Kruschke, John},
  date = {1992-01-01},
  journaltitle = {Psychological review},
  shortjournal = {Psychological review},
  volume = {99},
  pages = {22--44},
  doi = {10.1037/0033-295X.99.1.22},
  abstract = {ALCOVE (attention learning covering map) is a connectionist model of category learning that incorporates an exemplar-based representation (Medin \& Schaffer, 1978; Nosofsky, 1986) with error-driven learning (Gluck \& Bower, 1988; Rumelhart, Hinton, \& Williams, 1986). Alcove selectively attends to relevant stimulus dimensions, is sensitive to correlated dimensions, can account for a form of base-rate neglect, does not suffer catastrophic forgetting, and can exhibit 3-stage (U-shaped) learning of high-frequency exceptions to rules, whereas such effects are not easily accounted for by models using other combinations of representation and learning method.},
  file = {C:\Users\c21012241\Zotero\storage\7AQUE2JD\ALCOVE - An Exemplar-based connectionist model of category learning - JK Kruschke - 1992.pdf}
}

@inproceedings{kumarAttributeSimileClassifiers2009,
  title = {Attribute and Simile Classifiers for Face Verification},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Kumar, Neeraj and Berg, Alexander C and Belhumeur, Peter N and Nayar, Shree K},
  date = {2009-09},
  pages = {365--372},
  publisher = {{IEEE}},
  location = {{Kyoto}},
  doi = {10.1109/ICCV.2009.5459250},
  url = {http://ieeexplore.ieee.org/document/5459250/},
  urldate = {2023-08-17},
  abstract = {We present two novel methods for face verification. Our first method – “attribute” classifiers – uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method – “simile” classifiers – removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92\% and 26.34\%, respectively, and 31.68\% when combined. For further testing across pose, illumination, and expression, we introduce a new data set – termed PubFig – of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.},
  eventtitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4244-4420-5},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\9M5EHPC7\Kumar et al. - 2009 - Attribute and simile classifiers for face verifica.pdf}
}

@online{lageLearningInterpretableConceptBased2020,
  title = {Learning {{Interpretable Concept-Based Models}} with {{Human Feedback}}},
  author = {Lage, Isaac and Doshi-Velez, Finale},
  date = {2020-12-04},
  eprint = {2012.02898},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2012.02898},
  urldate = {2023-09-02},
  abstract = {Machine learning models that first learn a representation of a domain in terms of human-understandable concepts, then use it to make predictions, have been proposed to facilitate interpretation and interaction with models trained on high-dimensional data. However these methods have important limitations: the way they define concepts are not inherently interpretable, and they assume that concept labels either exist for individual instances or can easily be acquired from users. These limitations are particularly acute for high-dimensional tabular features. We propose an approach for learning a set of transparent concept definitions in high-dimensional tabular data that relies on users labeling concept features instead of individual instances. Our method produces concepts that both align with users’ intuitive sense of what a concept means, and facilitate prediction of the downstream label by a transparent machine learning model. This ensures that the full model is transparent and intuitive, and as predictive as possible given this constraint. We demonstrate with simulated user feedback on real prediction problems, including one in a clinical domain, that this kind of direct feedback is much more efficient at learning solutions that align with ground truth concept definitions than alternative transparent approaches that rely on labeling instances or other existing interaction mechanisms, while maintaining similar predictive performance.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\496K9N9I\Lage and Doshi-Velez - 2020 - Learning Interpretable Concept-Based Models with H.pdf}
}

@inproceedings{lampertLearningDetectUnseen2009,
  title = {Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lampert, Christoph H. and Nickisch, Hannes and Harmeling, Stefan},
  date = {2009-06},
  pages = {951--958},
  publisher = {{IEEE}},
  location = {{Miami, FL}},
  doi = {10.1109/CVPR.2009.5206594},
  url = {https://ieeexplore.ieee.org/document/5206594/},
  urldate = {2023-08-17},
  abstract = {We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels.},
  eventtitle = {2009 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPR Workshops}})},
  isbn = {978-1-4244-3992-8},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\QNIRPFKN\Lampert et al. - 2009 - Learning to detect unseen object classes by betwee.pdf}
}

@online{larsonHowWeAnalyzed2016,
  title = {How {{We Analyzed}} the {{COMPAS Recidivism Algorithm}}},
  author = {Larson, Jeff and Mattu, Surya and Kirchner, Lauren and Angwin, Julia},
  date = {2016-05-23},
  url = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm},
  urldate = {2023-08-28},
  abstract = {ProPublica is an independent, non-profit newsroom that produces investigative journalism in the public interest.},
  langid = {english},
  organization = {{ProPublica}},
  file = {C:\Users\c21012241\Zotero\storage\FA6TVHHD\how-we-analyzed-the-compas-recidivism-algorithm.html}
}

@article{lazarAISafetyWhose2023,
  title = {{{AI}} Safety on Whose Terms?},
  author = {Lazar, Seth and Nelson, Alondra},
  date = {2023-07-14},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {381},
  number = {6654},
  pages = {138--138},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.adi8982},
  url = {https://www.science.org/doi/10.1126/science.adi8982},
  urldate = {2023-10-09},
  abstract = {Rapid, widespread adoption of the latest large language models has sparked both excitement and concern about advanced artificial intelligence (AI). In response, many are looking to the field of AI safety for answers. Major AI companies are purportedly investing heavily in this young research program, even as they cut “trust and safety” teams addressing harms from current systems. Governments are taking notice too. The United Kingdom just invested £100 million in a new “Foundation Model Taskforce” and plans an AI safety summit this year. And yet, as research priorities are being set, it is already clear that the prevailing technical agenda for AI safety is inadequate to address critical questions. Only a sociotechnical approach can truly limit current and potential dangers of advanced AI.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\SRHC7DFQ\Lazar and Nelson - 2023 - AI safety on whose terms.pdf}
}

@incollection{lazarPowerAINature2022,
  title = {Power and {{AI}}: {{Nature}} and {{Justification}}},
  shorttitle = {Power and {{AI}}},
  booktitle = {The {{Oxford Handbook}} of {{AI Governance}}},
  author = {Lazar, Seth},
  editor = {Bullock, Justin B. and Chen, Yu-Che and Himmelreich, Johannes and Hudson, Valerie M. and Korinek, Anton and Young, Matthew M. and Zhang, Baobao},
  date = {2022},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780197579329.013.12},
  url = {https://doi.org/10.1093/oxfordhb/9780197579329.013.12},
  urldate = {2023-10-13},
  abstract = {AI and related computational systems are being used by some to exercise power over others. This is especially clear in our online lives, which are increasingly structured and governed by computational systems using some of the most advanced techniques in AI. But it is also apparent in our offline lives, as computational systems using AI are used by powerful actors, including states, local government, and employers. Proponents of various principles of “AI Ethics” sometimes imply that the sole normative function of those principles is to ensure that AI is used to achieve socially acceptable goals. Drawing attention to the ways in which AI systems are used to exercise power demonstrates the inadequacy of this normative analysis. When new and intensified power relations develop, we must attend not only to what power is used for, but also to how and by whom it is used.},
  isbn = {978-0-19-757932-9},
  file = {C\:\\Users\\c21012241\\Zotero\\storage\\VZWNJULF\\Lazar - Power and AI Nature and Justification.pdf;C\:\\Users\\c21012241\\Zotero\\storage\\3MTYVWWG\\355437737.html}
}

@article{leeTrustAutomationDesigning2004a,
  title = {Trust in {{Automation}}: {{Designing}} for {{Appropriate Reliance}}},
  shorttitle = {Trust in {{Automation}}},
  author = {Lee, John D. and See, Katrina A.},
  date = {2004-03-01},
  journaltitle = {Human Factors},
  shortjournal = {Hum Factors},
  volume = {46},
  number = {1},
  pages = {50--80},
  publisher = {{SAGE Publications Inc}},
  issn = {0018-7208},
  doi = {10.1518/hfes.46.1.50_30392},
  url = {https://journals.sagepub.com/doi/abs/10.1518/hfes.46.1.50_30392},
  urldate = {2023-10-12},
  abstract = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\R3DSK2WV\Lee and See - 2004 - Trust in Automation Designing for Appropriate Rel.pdf}
}

@article{leichtmannEffectsExplainableArtificial2023,
  title = {Effects of {{Explainable Artificial Intelligence}} on Trust and Human Behavior in a High-Risk Decision Task},
  author = {Leichtmann, Benedikt and Humer, Christina and Hinterreiter, Andreas and Streit, Marc and Mara, Martina},
  date = {2023-02},
  journaltitle = {Computers in Human Behavior},
  shortjournal = {Computers in Human Behavior},
  volume = {139},
  pages = {107539},
  issn = {07475632},
  doi = {10.1016/j.chb.2022.107539},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563222003594},
  urldate = {2023-08-06},
  abstract = {Understanding the recommendations of an artificial intelligence (AI) based assistant for decision-making is especially important in high-risk tasks, such as deciding whether a mushroom is edible or poisonous. To foster user understanding and appropriate trust in such systems, we assessed the effects of explainable artificial intelligence (XAI) methods and an educational intervention on AI-assisted decision-making behavior in a 2 × 2 between subjects online experiment with �� = 410 participants. We developed a novel use case in which users go on a virtual mushroom hunt and are tasked with picking edible and leaving poisonous mushrooms. Users were provided with an AI-based app that showed classification results of mushroom images. To manipulate explainability, one subgroup additionally received attribution-based and example-based explanations of the AI’s predictions; for the educational intervention one subgroup received additional information on how the AI worked. We found that the group that received explanations outperformed that which did not and showed better calibrated trust levels. Contrary to our expectations, we found that the educational intervention, domainspecific (i.e., mushroom) knowledge, and AI knowledge had no effect on performance. We discuss practical implications and introduce the mushroom-picking task as a promising use case for XAI research.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\XT9FLVZA\Leichtmann et al. - 2023 - Effects of Explainable Artificial Intelligence on .pdf}
}

@article{liaoUnsupervisedLearningReveals2023,
  title = {Unsupervised Learning Reveals Interpretable Latent Representations for Translucency Perception},
  author = {Liao, Chenxi and Sawayama, Masataka and Xiao, Bei},
  editor = {Fleming, Roland W.},
  date = {2023-02-08},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {19},
  number = {2},
  pages = {e1010878},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010878},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1010878},
  urldate = {2023-08-06},
  abstract = {Humans constantly assess the appearance of materials to plan actions, such as stepping on icy roads without slipping. Visual inference of materials is important but challenging because a given material can appear dramatically different in various scenes. This problem especially stands out for translucent materials, whose appearance strongly depends on lighting, geometry, and viewpoint. Despite this, humans can still distinguish between different materials, and it remains unsolved how to systematically discover visual features pertinent to material inference from natural images. Here, we develop an unsupervised style-based image generation model to identify perceptually relevant dimensions for translucent material appearances from photographs. We find our model, with its layer-wise latent representation, can synthesize images of diverse and realistic materials. Importantly, without supervision, human-understandable scene attributes, including the object’s shape, material, and body color, spontaneously emerge in the model’s layer-wise latent space in a scale-specific manner. By embedding an image into the learned latent space, we can manipulate specific layers’ latent code to modify the appearance of the object in the image. Specifically, we find that manipulation on the early-layers (coarse spatial scale) transforms the object’s shape, while manipulation on the later-layers (fine spatial scale) modifies its body color. The middle-layers of the latent space selectively encode translucency features and manipulation of such layers coherently modifies the translucency appearance, without changing the object’s shape or body color. Moreover, we find the middle-layers of the latent space can successfully predict human translucency ratings, suggesting that translucent impressions are established in mid-to-low spatial scale features. This layer-wise latent representation allows us to systematically discover perceptually relevant image features for human translucency perception. Together, our findings reveal that learning the scale-specific statistical structure of natural images might be crucial for humans to efficiently represent material properties across contexts.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\88SB9HCX\Liao et al. - 2023 - Unsupervised learning reveals interpretable latent.pdf}
}

@online{loschInterpretabilityClassificationOutput2019,
  title = {Interpretability {{Beyond Classification Output}}: {{Semantic Bottleneck Networks}}},
  shorttitle = {Interpretability {{Beyond Classification Output}}},
  author = {Losch, Max and Fritz, Mario and Schiele, Bernt},
  date = {2019-07-28},
  eprint = {1907.10882},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1907.10882},
  urldate = {2023-08-24},
  abstract = {Today’s deep learning systems deliver high performance based on end-to-end training. While they deliver strong performance, these systems are hard to interpret. To address this issue, we propose Semantic Bottleneck Networks (SBN): deep networks with semantically interpretable intermediate layers that all downstream results are based on. As a consequence, the analysis on what the final prediction is based on is transparent to the engineer and failure cases and modes can be analyzed and avoided by high-level reasoning. We present a case study on street scene segmentation to demonstrate the feasibility and power of SBN. In particular, we start from a well performing classic deep network which we adapt to house a SB-Layer containing task related semantic concepts (such as object-parts and materials). Importantly, we can recover state of the art performance despite a drastic dimensionality reduction from 1000s (non-semantic feature) to 10s (semantic concept) channels. Additionally we show how the activations of the SB-Layer can be used for both the interpretation of failure cases of the network as well as for confidence prediction of the resulting output. For the first time, e.g., we show interpretable segmentation results for most predictions at over 99\% accuracy.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\4EHD5Z5K\Losch et al. - 2019 - Interpretability Beyond Classification Output Sem.pdf}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  date = {2017},
  location = {{Long Beach, CA, USA.}},
  url = {https://www.researchgate.net/publication/317062430_A_Unified_Approach_to_Interpreting_Model_Predictions},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  eventtitle = {31st {{Conference}} on {{Neural Information Processing Systems}}},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\3UVQCNJ8\Lundberg and Lee - A Unified Approach to Interpreting Model Predictio.pdf}
}

@online{mahinpeiPromisesPitfallsBlackBox2021,
  title = {Promises and {{Pitfalls}} of {{Black-Box Concept Learning Models}}},
  author = {Mahinpei, Anita and Clark, Justin and Lage, Isaac and Doshi-Velez, Finale and Pan, Weiwei},
  date = {2021-06-24},
  eprint = {2106.13314},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.13314},
  urldate = {2023-09-03},
  abstract = {Machine learning models that incorporate concept learning as an intermediate step in their decision making process can match the performance of black-box predictive models while retaining the ability to explain outcomes in human understandable terms. However, we demonstrate that the concept representations learned by these models encode information beyond the pre-defined concepts, and that natural mitigation strategies do not fully work, rendering the interpretation of the downstream prediction misleading. We describe the mechanism underlying the information leakage and suggest recourse for mitigating its effects.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\QJK3WYQY\Mahinpei et al. - 2021 - Promises and Pitfalls of Black-Box Concept Learnin.pdf}
}

@online{margeloiuConceptBottleneckModels2021,
  title = {Do {{Concept Bottleneck Models Learn}} as {{Intended}}?},
  author = {Margeloiu, Andrei and Ashman, Matthew and Bhatt, Umang and Chen, Yanzhi and Jamnik, Mateja and Weller, Adrian},
  date = {2021-05-10},
  eprint = {2105.04289},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2105.04289},
  urldate = {2023-08-17},
  abstract = {Concept bottleneck models map from raw inputs to concepts, and then from concepts to targets. Such models aim to incorporate pre-specified, high-level concepts into the learning procedure, and have been motivated to meet three desiderata: interpretability, predictability, and intervenability. However, we find that concept bottleneck models struggle to meet these goals. Using post hoc interpretability methods, we demonstrate that concepts do not correspond to anything semantically meaningful in input space, thus calling into question the usefulness of concept bottleneck models in their current form.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\GZP5CT4D\Margeloiu et al. - 2021 - Do Concept Bottleneck Models Learn as Intended.pdf}
}

@book{martelMedicalImageComputing2020,
  title = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} – {{MICCAI}} 2020: 23rd {{International Conference}}, {{Lima}}, {{Peru}}, {{October}} 4–8, 2020, {{Proceedings}}, {{Part I}}},
  shorttitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} – {{MICCAI}} 2020},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12261},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-59710-8},
  url = {https://link.springer.com/10.1007/978-3-030-59710-8},
  urldate = {2023-08-06},
  isbn = {978-3-030-59709-2 978-3-030-59710-8},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\I4BHKKC2\Martel et al. - 2020 - Medical Image Computing and Computer Assisted Inte.pdf}
}

@online{martensTellMeStory2023,
  title = {Tell {{Me}} a {{Story}}! {{Narrative-Driven XAI}} with {{Large Language Models}}},
  author = {Martens, David and Dams, Camille and Hinns, James and Vergouwen, Mark},
  date = {2023-09-29},
  eprint = {2309.17057},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.17057},
  urldate = {2023-10-09},
  abstract = {In today’s critical domains, the predominance of black-box machine learning models amplifies the demand for Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present ‘what ifs’ but leave users grappling with the ‘why’. To bridge this gap, we introduce XAIstories. Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF explanations to explain a decision. Our results are striking: over 90\% of the surveyed general audience finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 92\% of data scientists indicating that it will contribute to the ease and confidence of nonspecialists in understanding AI predictions. Additionally, 83\% of data scientists indicate they are likely to use SHAPstories for this purpose. In image classification, CFstories are considered more or equally convincing as users own crafted stories by over 75\% of lay user participants. CFstories also bring a tenfold speed gain in creating a narrative, and improves accuracy by over 20\% compared to manually created narratives. The results thereby suggest that XAIstories may provide the missing link in truly explaining and understanding AI predictions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C:\Users\c21012241\Zotero\storage\A6JM449D\Martens et al. - 2023 - Tell Me a Story! Narrative-Driven XAI with Large L.pdf}
}

@article{martinSelectivityNeuralNetworks2021,
  title = {Selectivity in {{Neural Networks}}},
  author = {Martin, Nicholas},
  date = {2021},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\B76XGDXS\Martin - Selectivity in Neural Networks.pdf}
}

@article{medinContextTheoryClassification1978,
  title = {Context {{Theory}} of {{Classification Learning}}},
  author = {Medin, Douglas L and Schaffer, Marguerite M and College, Barnard},
  date = {1978},
  journaltitle = {Psychological Review},
  volume = {85},
  number = {3},
  pages = {207--238},
  doi = {10.1037/0033-295X.85.3.207},
  url = {https://groups.psych.northwestern.edu/medin/documents/MedinSchaffer1978PsychRev.pdf},
  abstract = {Most theories dealing with ill-defined concepts assume that performance is based on category level information or a mixture of category level and specific item information. A context theory of classification is described in which judgments are assumed to derive exclusively from stored exemplar information. The main idea is that a probe item acts as a retrieval cue to access information associated with stimuli similar to the probe. The predictions of the context theory are contrasted with those of a class of theories (including prototype theory) that assume that the information entering into judgments can be derived from an additive combination of information from component cue dimensions. Across 4 experiments with 128 paid Ss, using both geometric forms and schematic faces as stimuli, the context theory consistently gave a better account of the data. The relation of context theory to other theories and phenomena associated with ill-defined concepts is discussed in detail.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\HJ7594WQ\Medin et al. - Context Theory of Classification Learning.pdf}
}

@book{mehtaExplainableAIFoundations2023,
  title = {Explainable {{AI}}: {{Foundations}}, {{Methodologies}} and {{Applications}}},
  shorttitle = {Explainable {{AI}}},
  editor = {Mehta, Mayuri and Palade, Vasile and Chatterjee, Indranath},
  date = {2023},
  series = {Intelligent {{Systems Reference Library}}},
  volume = {232},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-12807-3},
  url = {https://link.springer.com/10.1007/978-3-031-12807-3},
  urldate = {2023-10-06},
  isbn = {978-3-031-12806-6 978-3-031-12807-3},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\I9S4K2Z3\Mehta et al. - 2023 - Explainable AI Foundations, Methodologies and App.pdf}
}

@inproceedings{melisRobustInterpretabilitySelfExplaining2018,
  title = {Towards {{Robust Interpretability}} with {{Self-Explaining Neural Networks}}},
  booktitle = {32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2018), {{Montréal}}, {{Canada}}.},
  author = {Melis, David Alvarez and Jaakkola, Tommi},
  date = {2018},
  abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
  eventtitle = {32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2018), {{Montréal}}, {{Canada}}.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\APIBTPX3\Melis and Jaakkola - Towards Robust Interpretability with Self-Explaini.pdf}
}

@online{metaEnforcingManipulatedMedia2020,
  title = {Enforcing {{Against Manipulated Media}}},
  author = {Meta},
  date = {2020-01-07T04:30:28+00:00},
  url = {https://about.fb.com/news/2020/01/enforcing-against-manipulated-media/},
  urldate = {2023-08-29},
  abstract = {We're strengthening our policy toward misleading manipulated videos that have been identified as deepfakes.},
  langid = {american},
  organization = {{Meta}},
  file = {C:\Users\c21012241\Zotero\storage\ZN6ZRP3X\enforcing-against-manipulated-media.html}
}

@article{miyatsuFeatureHighlightingEnhances2019,
  title = {Feature Highlighting Enhances Learning of a Complex Natural-Science Category.},
  author = {Miyatsu, Toshiya and Gouravajhala, Reshma and Nosofsky, Robert M. and McDaniel, Mark A.},
  date = {2019-01},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  shortjournal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {45},
  number = {1},
  pages = {1--16},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000538},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xlm0000538},
  urldate = {2023-10-10},
  abstract = {Learning naturalistic categories, which tend to have fuzzy boundaries and vary on many dimensions, can often be harder than learning well defined categories. One method for facilitating the category learning of naturalistic stimuli may be to provide explicit feature descriptions that highlight the characteristic features of each category. Although this method is commonly used in textbooks and classrooms, theoretically it remains uncertain whether feature descriptions should advantage learning complex natural-science categories. In three experiments, participants were trained on 12 categories of rocks, either without or with a brief description highlighting key features of each category. After training, they were tested on their ability to categorize both old and new rocks from each of the categories. Providing feature descriptions as a caption under a rock image failed to improve category learning relative to providing only the rock image with its category label (Experiment 1). However, when these same feature descriptions were presented such that they were explicitly linked to the relevant parts of the rock image (feature highlighting), participants showed significantly higher performance on both immediate generalization to new rocks (Experiment 2) and generalization after a 2-day delay (Experiment 3). Theoretical and practical implications are discussed.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\KQZ5L3MT\Miyatsu et al. - 2019 - Feature highlighting enhances learning of a comple.pdf}
}

@article{mohseniMultidisciplinarySurveyFramework2021,
  title = {A {{Multidisciplinary Survey}} and {{Framework}} for {{Design}} and {{Evaluation}} of {{Explainable AI Systems}}},
  author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
  date = {2021-12-31},
  journaltitle = {ACM Transactions on Interactive Intelligent Systems},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  volume = {11},
  number = {3-4},
  pages = {1--45},
  issn = {2160-6455, 2160-6463},
  doi = {10.1145/3387166},
  url = {https://dl.acm.org/doi/10.1145/3387166},
  urldate = {2023-10-09},
  abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of               artificial intelligence               (               AI               ) applications used in everyday life.               Explainable AI               (               XAI               ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\9NWYLMVA\Mohseni et al. - 2021 - A Multidisciplinary Survey and Framework for Desig.pdf}
}

@online{mundhenkEfficientSaliencyMaps2020,
  title = {Efficient {{Saliency Maps}} for {{Explainable AI}}},
  author = {Mundhenk, T. Nathan and Chen, Barry Y. and Friedland, Gerald},
  date = {2020-03-09},
  eprint = {1911.11293},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.11293},
  urldate = {2023-08-15},
  abstract = {We describe an explainable AI saliency map method for use with deep convolutional neural networks (CNN) that is much more efficient than popular fine-resolution gradient methods. It is also quantitatively similar or better in accuracy. Our technique works by measuring information at the end of each network scale which is then combined into a single saliency map. We describe how saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. We visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information. This provides an interesting comparison of scale information contributions within the network not provided by other saliency map methods. Using our method instead of Guided Backprop, coarse-resolution class activation methods such as Grad-CAM and GradCAM++ seem to yield demonstrably superior results without sacrificing speed. This will make fine-resolution saliency methods feasible on resource limited platforms such as robots, cell phones, low-cost industrial devices, astronomy and satellite imagery3.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Performance,Computer Science - Robotics},
  file = {C:\Users\c21012241\Zotero\storage\GDVP7WJS\Mundhenk et al. - 2020 - Efficient Saliency Maps for Explainable AI.pdf}
}

@inproceedings{NEURIPS2019_3e9f0fc9,
  title = {Defending against Neural Fake News},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Buc, given=F., prefix=dAlché-, useprefix=true and Fox, E. and Garnett, R.},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf},
  file = {C:\Users\c21012241\Zotero\storage\RG738ALL\Zellers et al. - Defending Against Neural Fake News.pdf}
}

@book{nixonFeatureExtractionImage2010,
  title = {Feature Extraction and Image Processing},
  author = {Nixon, Mark S. and Aguado, Alberto S.},
  date = {2010},
  edition = {2. ed., reprinted},
  publisher = {{Acad. Press}},
  location = {{Amsterdam}},
  isbn = {978-0-12-372538-7},
  langid = {english},
  pagetotal = {406},
  file = {C:\Users\c21012241\Zotero\storage\ITDMAB75\Nixon and Aguado - 2010 - Feature extraction and image processing.pdf}
}

@article{nosofskyAttentionSimilarityIdentificationCategorization1986a,
  title = {Attention, {{Similarity}}, and the {{Identification-Categorization Relationship}}},
  author = {Nosofsky, Robert},
  date = {1986-03-01},
  journaltitle = {Journal of experimental psychology. General},
  shortjournal = {Journal of experimental psychology. General},
  volume = {115},
  pages = {39--61},
  doi = {10.1037/0096-3445.115.1.39},
  abstract = {A unified quantitative approach to modeling subjects' identification and categorization of multidimensional perceptual stimuli is proposed and tested. Two subjects identified and categorized the same set of perceptually confusable stimuli varying on separable dimensions. The identification data were modeled using Shepard's (1957) multidimensional scaling-choice framework. This framework was then extended to model the subjects' categorization performance. The categorization model, which generalizes the context theory of classification developed by Medin and Schaffer (1978), assumes that subjects store category exemplars in memory. Classification decisions are based on the similarity of stimuli to the stored exemplars. It is assumed that the same multidimensional perceptual representation underlies performance in both the identification and categorization paradigms. However, because of the influence of selective attention, similarity relationships change systematically across the two paradigms. Some support was gained for the hypothesis that subjects distribute attention among component dimensions so as to optimize categorization performance. Evidence was also obtained that subjects may have augmented their category representations with inferred exemplars. Implications of the results for theories of multidimensional scaling and categorization are discussed.},
  file = {C:\Users\c21012241\Zotero\storage\PIP39Z8G\Nosofsky - Attention, Similarity, and the Identification-Cate.pdf}
}

@article{nosofskyDevelopmentFeaturespaceRepresentation2018,
  title = {Toward the Development of a Feature-Space Representation for a Complex Natural Category Domain},
  author = {Nosofsky, Robert M. and Sanders, Craig A. and Meagher, Brian J. and Douglas, Bruce J.},
  date = {2018-04},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {50},
  number = {2},
  pages = {530--556},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0884-8},
  url = {http://link.springer.com/10.3758/s13428-017-0884-8},
  urldate = {2023-08-25},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\MYUHYL28\Nosofsky et al. - 2018 - Toward the development of a feature-space represen.pdf}
}

@article{nosofskyLearningNaturalScienceCategories2017,
  title = {On {{Learning Natural-Science Categories That Violate}} the {{Family-Resemblance Principle}}},
  author = {Nosofsky, Robert M. and Sanders, Craig A. and Gerdom, Alex and Douglas, Bruce J. and McDaniel, Mark A.},
  date = {2017-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {28},
  number = {1},
  pages = {104--114},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797616675636},
  url = {http://journals.sagepub.com/doi/10.1177/0956797616675636},
  urldate = {2023-08-25},
  abstract = {The general view in psychological science is that natural categories obey a coherent, family-resemblance principle. In this investigation, we documented an example of an important exception to this principle: Results of a multidimensionalscaling study of igneous, metamorphic, and sedimentary rocks (Experiment 1) suggested that the structure of these categories is disorganized and dispersed. This finding motivated us to explore what might be the optimal procedures for teaching dispersed categories, a goal that is likely critical to science education in general. Subjects in Experiment 2 learned to classify pictures of rocks into compact or dispersed high-level categories. One group learned the categories through focused high-level training, whereas a second group was required to simultaneously learn classifications at a subtype level. Although high-level training led to enhanced performance when the categories were compact, subtype training was better when the categories were dispersed. We provide an interpretation of the results in terms of an exemplar-memory model of category learning.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\72PI5K3Q\Nosofsky et al. - 2017 - On Learning Natural-Science Categories That Violat.pdf}
}

@article{nosofskyTestsExemplarmemoryModel2018,
  title = {Tests of an Exemplar-Memory Model of Classification Learning in a High-Dimensional Natural-Science Category Domain.},
  author = {Nosofsky, Robert M. and Sanders, Craig A. and McDaniel, Mark A.},
  date = {2018-03},
  journaltitle = {Journal of Experimental Psychology: General},
  shortjournal = {Journal of Experimental Psychology: General},
  volume = {147},
  number = {3},
  pages = {328--353},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000369},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000369},
  urldate = {2023-08-25},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\MG6RDMEU\Nosofsky et al. - 2018 - Tests of an exemplar-memory model of classificatio.pdf}
}

@online{openaiJukebox2023,
  title = {Jukebox},
  author = {OpenAI},
  date = {2023},
  url = {https://openai.com/research/jukebox},
  urldate = {2023-08-29},
  abstract = {We’re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We’re releasing the model weights and code, along with a tool to explore the generated~samples.},
  langid = {american},
  file = {C:\Users\c21012241\Zotero\storage\8XYJQY5X\jukebox.html}
}

@book{pashentsevPalgraveHandbookMalicious2023,
  title = {The {{Palgrave Handbook}} of {{Malicious Use}} of {{AI}} and {{Psychological Security}}},
  editor = {Pashentsev, Evgeny},
  date = {2023},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-22552-9},
  url = {https://link.springer.com/10.1007/978-3-031-22552-9},
  urldate = {2023-08-29},
  isbn = {978-3-031-22551-2 978-3-031-22552-9},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\7C9MGKSX\Pashentsev - 2023 - The Palgrave Handbook of Malicious Use of AI and P.pdf}
}

@inproceedings{perrigTrustIssuesTrust2023,
  title = {Trust {{Issues}} with {{Trust Scales}}: {{Examining}} the {{Psychometric Quality}} of {{Trust Measures}} in the {{Context}} of {{AI}}},
  shorttitle = {Trust {{Issues}} with {{Trust Scales}}},
  booktitle = {Extended {{Abstracts}} of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Perrig, Sebastian A. C. and Scharowski, Nicolas and Brühlmann, Florian},
  date = {2023-04-19},
  pages = {1--7},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544549.3585808},
  url = {https://dl.acm.org/doi/10.1145/3544549.3585808},
  urldate = {2023-10-09},
  abstract = {Trust is crucial for human interaction with artifcial intelligence (AI) and is frequently measured through questionnaires or rating scales. One commonly used questionnaire in AI research is the Trust between People and Automation scale (TPA). However, its psychometric quality has yet to be examined in the context of AI. More recently, a Trust Scale for Explainable AI (TXAI) was recommended but not empirically evaluated. In this study, we assessed the psychometric qualities of both scales, using confrmatory and exploratory factor analyses to test the scales’ validity and coefcients ��� and ��� for reliability estimation. Our results suggested good psychometric quality for the TXAI after removing one item. Concerning the TPA, acceptable quality was only achieved when using a two-factor model (trust and distrust) and after removing two items. We provide recommendations for using the two scales and evidence to distinguish trust and distrust as separate psychological constructs.},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9422-2},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\XP2I9ULX\Perrig et al. - 2023 - Trust Issues with Trust Scales Examining the Psyc.pdf}
}

@article{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021},
  journaltitle = {Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021},
  doi = {10.48550/arXiv.2103.00020},
  abstract = {SOTA computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\DQ34G6A7\Radford et al. - Learning Transferable Visual Models From Natural L.pdf}
}

@inproceedings{raukerTransparentAISurvey2023,
  title = {Toward {{Transparent AI}}: {{A Survey}} on {{Interpreting}} the {{Inner Structures}} of {{Deep Neural Networks}}},
  shorttitle = {Toward {{Transparent AI}}},
  booktitle = {2023 {{IEEE Conference}} on {{Secure}} and {{Trustworthy Machine Learning}} ({{SaTML}})},
  author = {Räuker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  date = {2023-02},
  pages = {464--483},
  publisher = {{IEEE}},
  location = {{Raleigh, NC, USA}},
  doi = {10.1109/SaTML54575.2023.00039},
  url = {https://ieeexplore.ieee.org/document/10136140/},
  urldate = {2023-08-06},
  abstract = {The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, “inner” interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions.},
  eventtitle = {2023 {{IEEE Conference}} on {{Secure}} and {{Trustworthy Machine Learning}} ({{SaTML}})},
  isbn = {978-1-66546-299-0},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\SFCI3MPT\Räuker et al. - 2023 - Toward Transparent AI A Survey on Interpreting th.pdf}
}

@article{ribeiroAnchorsHighPrecisionModelAgnostic2018,
  title = {Anchors: {{High-Precision Model-Agnostic Explanations}}},
  shorttitle = {Anchors},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2018-04-25},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11491},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
  urldate = {2023-08-24},
  abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufficient” conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\Y6ILJ9SS\Ribeiro et al. - 2018 - Anchors High-Precision Model-Agnostic Explanation.pdf}
}

@online{ribeiroModelAgnosticInterpretabilityMachine2016,
  title = {Model-{{Agnostic Interpretability}} of {{Machine Learning}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-06-16},
  eprint = {1606.05386},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.05386},
  urldate = {2023-08-24},
  abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\YZPM5WUI\Ribeiro et al. - 2016 - Model-Agnostic Interpretability of Machine Learnin.pdf}
}

@online{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-09},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2023-08-15},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\M45H59TU\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{rudinAgeSecrecyUnfairness2020,
  title = {The {{Age}} of {{Secrecy}} and {{Unfairness}} in {{Recidivism Prediction}}},
  author = {Rudin, Cynthia and Wang, Caroline and Coker, Beau},
  date = {2020-01-31},
  journaltitle = {Harvard Data Science Review},
  shortjournal = {Harvard Data Science Review},
  volume = {2},
  number = {1},
  doi = {10.1162/99608f92.6ed64b30},
  url = {https://hdsr.mitpress.mit.edu/pub/7z10o269},
  urldate = {2023-08-28},
  abstract = {In our current society, secret algorithms make important decisions about individuals. There has been substantial discussion about whether these algorithms are unfair to groups of individuals. While noble, this pursuit is complex and ultimately stagnating because there is no clear definition of fairness and competing definitions are largely incompatible. We argue that the focus on the question of fairness is misplaced, as these algorithms fail to meet a more important and yet readily obtainable goal: transparency. As a result, creators of secret algorithms can provide incomplete or misleading descriptions about how their models work, and various other kinds of errors can easily go unnoticed. By trying to partially reconstruct the COMPAS model—a recidivism risk-scoring model used throughout the criminal justice system—we show that it does not seem to depend linearly on the defendant’s age, despite statements to the contrary by the model’s creator. This observation has not been made before despite many recently published papers on COMPAS. Furthermore, by subtracting from COMPAS its (hypothesized) nonlinear age component, we show that COMPAS does not necessarily depend on race other than through age and criminal history. This contradicts ProPublica’s analysis, which made assumptions about age that disagree with what we observe in the data. In other words, faulty assumptions about a proprietary model led to faulty conclusions that went unchecked until now. Were the model transparent in the first place, this likely would not have occurred. We demonstrate other issues with definitions of fairness and lack of transparency in the context of COMPAS, including that a simple model based entirely on a defendant’s age is as ‘unfair’ as COMPAS by ProPublica’s chosen definition. We find that there are many defendants with low risk scores but long criminal histories, suggesting that data inconsistencies occur frequently in criminal justice databases. We argue that transparency satisfies a different notion of procedural fairness by providing both the defendants and the public with the opportunity to scrutinize the methodology and calculations behind risk scores for recidivism.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\UWXKAFGH\Rudin et al. - 2020 - The Age of Secrecy and Unfairness in Recidivism Pr.pdf}
}

@online{rudinInterpretableMachineLearning2021,
  title = {Interpretable {{Machine Learning}}: {{Fundamental Principles}} and 10 {{Grand Challenges}}},
  shorttitle = {Interpretable {{Machine Learning}}},
  author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
  date = {2021-07-09},
  eprint = {2103.11251},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2103.11251},
  urldate = {2023-08-24},
  abstract = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the “Rashomon set” of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.},
  langid = {english},
  pubstate = {preprint},
  keywords = {68T01,Computer Science - Machine Learning,I.2.6,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\GI42SC2W\Rudin et al. - 2021 - Interpretable Machine Learning Fundamental Princi.pdf}
}

@online{rudinStopExplainingBlack2019,
  title = {Stop {{Explaining Black Box Machine Learning Models}} for {{High Stakes Decisions}} and {{Use Interpretable Models Instead}}},
  author = {Rudin, Cynthia},
  date = {2019-09-21},
  eprint = {1811.10154},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.10154},
  urldate = {2023-07-11},
  abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward – it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\T3M9HHAC\Rudin - 2019 - Stop Explaining Black Box Machine Learning Models .pdf}
}

@book{samekExplainableAIInterpreting2019,
  title = {Explainable {{AI}}: {{Interpreting}}, {{Explaining}} and {{Visualizing Deep Learning}}},
  shorttitle = {Explainable {{AI}}},
  editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11700},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-28954-6},
  url = {http://link.springer.com/10.1007/978-3-030-28954-6},
  urldate = {2023-07-11},
  isbn = {978-3-030-28953-9 978-3-030-28954-6},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\ZECA6FW7\Samek et al. - 2019 - Explainable AI Interpreting, Explaining and Visua.pdf}
}

@article{sandersTrainingDeepNetworks2020,
  title = {Training {{Deep Networks}} to {{Construct}} a {{Psychological Feature Space}} for a {{Natural-Object Category Domain}}},
  author = {Sanders, Craig A. and Nosofsky, Robert M.},
  date = {2020-09},
  journaltitle = {Computational Brain \& Behavior},
  shortjournal = {Comput Brain Behav},
  volume = {3},
  number = {3},
  pages = {229--251},
  issn = {2522-0861, 2522-087X},
  doi = {10.1007/s42113-020-00073-z},
  url = {http://link.springer.com/10.1007/s42113-020-00073-z},
  urldate = {2023-10-10},
  abstract = {Many successful formal models of human categorization have been developed, but these models have been tested almost exclusively using artificial categories, because deriving psychological representations of large sets of natural stimuli using traditional methods such as multidimensional scaling (MDS) has been an intractable task. Here, we propose a novel integration in which MDS representations are used to train deep convolutional neural networks (CNNs) to automatically derive psychological representations for unlimited numbers of natural stimuli. In an example application, we train an ensemble of CNNs to produce the MDS coordinates of images of rocks, and we show that the ensemble can predict the MDS coordinates of new sets of rocks, even those not part of the original MDS space. We then show that the CNN-predicted MDS representations, unlike offthe-shelf CNN representations, can be used in conjunction with a formal psychological model to predict human categorization behavior. We further show that the CNNs can be trained to produce additional dimensions that extend the original MDS space and provide even better model fits to human category-learning data. Our integrated method provides a promising approach that can be instrumental in allowing researchers to extend traditional psychological-scaling and category-learning models to the complex, high-dimensional domains that exist in the natural world.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\Y2DDJWQH\Sanders and Nosofsky - 2020 - Training Deep Networks to Construct a Psychologica.pdf}
}

@article{scheufeleScienceAudiencesMisinformation2019,
  title = {Science Audiences, Misinformation, and Fake News},
  author = {Scheufele, Dietram A. and Krause, Nicole M.},
  date = {2019-04-16},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {116},
  number = {16},
  pages = {7662--7669},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1805871115},
  url = {https://pnas.org/doi/full/10.1073/pnas.1805871115},
  urldate = {2023-08-26},
  abstract = {Concerns about public misinformation in the United States—ranging from politics to science—are growing. Here, we provide an overview of how and why citizens become (and sometimes remain) misinformed about science. Our discussion focuses specifically on misinformation among individual citizens. However, it is impossible to understand individual information processing and acceptance without taking into account social networks, information ecologies, and other macro-level variables that provide important social context. Specifically, we show how being misinformed is a function of a person’s ability and motivation to spot falsehoods, but also of other group-level and societal factors that increase the chances of citizens to be exposed to correct(ive) information. We conclude by discussing a number of research areas—some of which echo themes of the 2017 National Academies of Sciences, Engineering, and Medicine’s               Communicating Science Effectively               report—that will be particularly important for our future understanding of misinformation, specifically a systems approach to the problem of misinformation, the need for more systematic analyses of science communication in new media environments, and a (re)focusing on traditionally underserved audiences.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\RBTH694Y\Scheufele and Krause - 2019 - Science audiences, misinformation, and fake news.pdf}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2017-10},
  pages = {618--626},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.74},
  url = {http://ieeexplore.ieee.org/document/8237336/},
  urldate = {2023-10-10},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  file = {C:\Users\c21012241\Zotero\storage\RTWXIM9U\Grad-CAM_Visual_Explanations_from_Deep_Networks_via_Gradient-Based_Localization.pdf}
}

@incollection{shapley17ValueNPerson1953,
  title = {17. {{A Value}} for n-{{Person Games}}},
  booktitle = {Contributions to the {{Theory}} of {{Games}} ({{AM-28}}), {{Volume II}}},
  author = {Shapley, L. S.},
  editor = {Kuhn, Harold William and Tucker, Albert William},
  date = {1953-12-31},
  pages = {307--318},
  publisher = {{Princeton University Press}},
  doi = {10.1515/9781400881970-018},
  url = {https://www.degruyter.com/document/doi/10.1515/9781400881970-018/html},
  urldate = {2023-10-10},
  isbn = {978-1-4008-8197-0},
  file = {C:\Users\c21012241\Zotero\storage\VI2KLNF6\A value for n-person games - Shapley - 1952.pdf}
}

@online{shenInterpretabilityEvaluationBenchmark2022,
  title = {An {{Interpretability Evaluation Benchmark}} for {{Pre-trained Language Models}}},
  author = {Shen, Yaozong and Wang, Lijie and Chen, Ying and Xiao, Xinyan and Liu, Jing and Wu, Hua},
  date = {2022-07-28},
  eprint = {2207.13948},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.13948},
  urldate = {2023-10-10},
  abstract = {While pre-trained language models (LMs) have brought great improvements in many NLP tasks, there is increasing attention to explore capabilities of LMs and interpret their predictions. However, existing works usually focus only on a certain capability with some downstream tasks. There is a lack of datasets for directly evaluating the masked word prediction performance and the interpretability of pre-trained LMs. To fill in the gap, we propose a novel evaluation benchmark providing with both English and Chinese annotated data. It tests LMs abilities in multiple dimensions, i.e., grammar, semantics, knowledge, reasoning and computation. In addition, it provides carefully annotated token-level rationales that satisfy sufficiency and compactness. It contains perturbed instances for each original instance, so as to use the rationale consistency under perturbations as the metric for faithfulness, a perspective of interpretability. We conduct experiments on several widely-used pretrained LMs. The results show that they perform very poorly on the dimensions of knowledge and computation. And their plausibility in all dimensions is far from satisfactory, especially when the rationale is short. In addition, the pre-trained LMs we evaluated are not robust on syntax-aware data. We will release this evaluation benchmark at http://xyz, and hope it can facilitate the research progress of pre-trained LMs.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\c21012241\Zotero\storage\3JSGGHMR\Shen et al. - 2022 - An Interpretability Evaluation Benchmark for Pre-t.pdf}
}

@online{shinCloserLookIntervention2023,
  title = {A {{Closer Look}} at the {{Intervention Procedure}} of {{Concept Bottleneck Models}}},
  author = {Shin, Sungbin and Jo, Yohan and Ahn, Sungsoo and Lee, Namhoon},
  date = {2023-07-02},
  eprint = {2302.14260},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.14260},
  urldate = {2023-09-02},
  abstract = {Concept bottleneck models (CBMs) are a class of interpretable neural network models that predict the target response of a given input based on its high-level concepts. Unlike the standard end-to-end models, CBMs enable domain experts to intervene on the predicted concepts and rectify any mistakes at test time, so that more accurate task predictions can be made at the end. While such intervenability provides a powerful avenue of control, many aspects of the intervention procedure remain rather unexplored. In this work, we develop various ways of selecting intervening concepts to improve the intervention effectiveness and conduct an array of in-depth analyses as to how they evolve under different circumstances. Specifically, we find that an informed intervention strategy can reduce the task error more than ten times compared to the current baseline under the same amount of intervention counts in realistic settings, and yet, this can vary quite significantly when taking into account different intervention granularity. We verify our findings through comprehensive evaluations, not only on the standard real datasets, but also on synthetic datasets that we generate based on a set of different causal graphs. We further discover some major pitfalls of the current practices which, without a proper addressing, raise concerns on reliability and fairness of the intervention procedure.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\GZKZ2X2J\Shin et al. - 2023 - A Closer Look at the Intervention Procedure of Con.pdf}
}

@online{simonyanDeepConvolutionalNetworks2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2023-08-17},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\c21012241\Zotero\storage\BA5DERUK\Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf}
}

@online{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  shorttitle = {{{SmoothGrad}}},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
  date = {2017-06-12},
  eprint = {1706.03825},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.03825},
  urldate = {2023-09-02},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\2I3VXF9H\Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf}
}

@article{smithThirtyCategorizationResults2000,
  title = {Thirty Categorization Results in Search of a Model.},
  author = {Smith, David J. and Minda, John Paul},
  date = {2000},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  shortjournal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {26},
  number = {1},
  pages = {3--27},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/0278-7393.26.1.3},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.26.1.3},
  urldate = {2023-08-06},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\ISZT3RHZ\Smith and Minda - 2000 - Thirty categorization results in search of a model.pdf}
}

@software{SoftVCVITSSinging2023,
  title = {{{SoftVC VITS Singing Voice Conversion}}},
  date = {2023-08-29T16:49:27Z},
  origdate = {2023-03-10T09:31:09Z},
  url = {https://github.com/svc-develop-team/so-vits-svc},
  urldate = {2023-08-29},
  abstract = {SoftVC VITS Singing Voice Conversion},
  organization = {{MoeVoiceConversion}},
  keywords = {ai,audio-analysis,deep-learning,flow,generative-adversarial-network,pytorch,singing-voice-conversion,so-vits-svc,sovits,speech,variational-inference,vc,vits,voice,voice-changer,voice-conversion,voiceconversion}
}

@online{SouthKoreaPresidential2022,
  title = {South {{Korea}}’s {{Presidential Deepfake}}},
  date = {2022-03-29T03:48:00+00:00},
  url = {https://www.vastmindz.com/south-koreas-presidential-deepfake/},
  urldate = {2023-08-29},
  abstract = {Learn about Deep Fake technology and its possible misuse in South Korean politics. Get an inside look at the process behind creating realistic AI-generated videos.},
  langid = {british},
  file = {C:\Users\c21012241\Zotero\storage\ZT3JC4K5\south-koreas-presidential-deepfake.html}
}

@article{sovranoExplanatoryArtificialIntelligence2022,
  title = {Explanatory Artificial Intelligence ({{YAI}}): Human-Centered Explanations of Explainable {{AI}} and Complex Data},
  shorttitle = {Explanatory Artificial Intelligence ({{YAI}})},
  author = {Sovrano, Francesco and Vitali, Fabio},
  date = {2022-10-10},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-022-00872-x},
  url = {https://link.springer.com/10.1007/s10618-022-00872-x},
  urldate = {2023-08-06},
  abstract = {In this paper we introduce a new class of software tools engaged in delivering successful explanations of complex processes on top of basic Explainable AI (XAI) software systems. These tools, that we call cumulatively Explanatory AI (YAI) systems, enhance the quality of the basic output of a XAI by adopting a user-centred approach to explanation that can cater to the individual needs of the explainees with measurable improvements in usability. Our approach is based on Achinstein’s theory of explanations, where explaining is an illocutionary (i.e., broad yet pertinent and deliberate) act of pragmatically answering a question. Accordingly, user-centrality enters in the equation by considering that the overall amount of information generated by answering all questions can rapidly become overwhelming and that individual users may perceive the need to explore just a few of them. In this paper, we give the theoretical foundations of YAI, formally defining a user-centred explanatory tool and the space of all possible explanations, or explanatory space, generated by it. To this end, we frame the explanatory space as an hypergraph of knowledge and we identify a set of heuristics and properties that can help approximating a decomposition of it into a tree-like representation for efficient and user-centred explanation retrieval. Finally, we provide some old and new empirical results to support our theory, showing that explanations are more than textual or visual presentations of the sole information provided by a XAI.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\CLXSIT58\Sovrano and Vitali - 2022 - Explanatory artificial intelligence (YAI) human-c.pdf}
}

@online{SpeechifyCompleteAI2022,
  title = {Speechify | {{Complete AI Voice Studio}} | {{TTS}}, {{AI Voice Over}} \& {{More}}},
  date = {2022-05-04T09:25:39+00:00},
  url = {https://speechify.com/},
  urldate = {2023-09-12},
  abstract = {The leading text to speech app with millions of downloads on Chrome, iOS, \& Android. Hear the Internet on any device. Try Speechify for free today.},
  langid = {american}
}

@article{speerConceptNetOpenMultilingual2017,
  title = {{{ConceptNet}} 5.5: {{An Open Multilingual Graph}} of {{General Knowledge}}},
  shorttitle = {{{ConceptNet}} 5.5},
  author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  date = {2017-02-12},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {31},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v31i1.11164},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11164},
  urldate = {2023-10-10},
  abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\Z27AQ7W3\Speer et al. - 2017 - ConceptNet 5.5 An Open Multilingual Graph of Gene.pdf}
}

@online{springenbergStrivingSimplicityAll2015a,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  date = {2015-04-13},
  eprint = {1412.6806},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.6806},
  urldate = {2023-08-17},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding – and building on other recent work for finding simple network structures – we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\c21012241\Zotero\storage\XK5G5WHN\Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf}
}

@inproceedings{sundararajanAxiomaticAttributionDeep2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}, {{PMLR}} 70:3319-3328, 2017},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  date = {2017},
  url = {http://proceedings.mlr.press/v70/sundararajan17a.html},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\39DHVK4M\Sundararajan et al. - Axiomatic Attribution for Deep Networks.pdf}
}

@inproceedings{uenoTrustHumanAIInteraction2022,
  title = {Trust in {{Human-AI Interaction}}: {{Scoping Out Models}}, {{Measures}}, and {{Methods}}},
  shorttitle = {Trust in {{Human-AI Interaction}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems Extended Abstracts}}},
  author = {Ueno, Takane and Sawa, Yuto and Kim, Yeongdae and Urakami, Jacqueline and Oura, Hiroki and Seaborn, Katie},
  date = {2022-04-27},
  pages = {1--7},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491101.3519772},
  url = {https://dl.acm.org/doi/10.1145/3491101.3519772},
  urldate = {2023-10-09},
  abstract = {Trust has emerged as a key factor in people’s interactions with AIinfused systems. Yet, little is known about what models of trust have been used and for what systems: robots, virtual characters, smart vehicles, decision aids, or others. Moreover, there is yet no known standard approach to measuring trust in AI. This scoping review maps out the state of affairs on trust in human-AI interaction (HAII) from the perspectives of models, measures, and methods. Findings suggest that trust is an important and multi-faceted topic of study within HAII contexts. However, most work is under-theorized and under-reported, generally not using established trust models and missing details about methods, especially Wizard of Oz. We offer several targets for systematic review work as well as a research agenda for combining the strengths and addressing the weaknesses of the current literature.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9156-6},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\7N4X5H7K\Ueno et al. - 2022 - Trust in Human-AI Interaction Scoping Out Models,.pdf}
}

@article{vanprooijenInfluenceControlBelief2015,
  title = {The {{Influence}} of {{Control}} on {{Belief}} in {{Conspiracy Theories}}: {{Conceptual}} and {{Applied Extensions}}: {{Control}} and Conspiracy Belief},
  shorttitle = {The {{Influence}} of {{Control}} on {{Belief}} in {{Conspiracy Theories}}},
  author = {Van Prooijen, Jan-Willem and Acker, Michele},
  date = {2015-09},
  journaltitle = {Applied Cognitive Psychology},
  shortjournal = {Appl. Cognit. Psychol.},
  volume = {29},
  number = {5},
  pages = {753--761},
  issn = {08884080},
  doi = {10.1002/acp.3161},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/acp.3161},
  urldate = {2023-08-26},
  abstract = {Threats to control have been found to increase belief in conspiracy theories. We argue, however, that previous research observing this effect was limited in two ways. First, previous research did not exclude the possibility that affirming control might reduce conspiracy beliefs. Second, because of artificial lab procedures, previous findings provide little information about the external validity of the control threat–conspiracy belief relationship. In Study 1, we address the first limitation and find that affirming control indeed reduces belief in conspiracy theories as compared with a neutral baseline condition. In Study 2, we address the second limitation of the literature. In a large-scale US sample, we find that a societal threat to control, that citizens actually experienced, predicts belief in a range of common conspiracy theories. Taken together, these findings increase insight in the fundamental relationship between the human need for control and the tendency to believe in conspiracy theories. Copyright © 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\SJ88S4AP\Van Prooijen and Acker - 2015 - The Influence of Control on Belief in Conspiracy T.pdf}
}

@article{vanprooijenWhyEducationPredicts2017,
  title = {Why {{Education Predicts Decreased Belief}} in {{Conspiracy Theories}}: {{Education}} and {{Conspiracy Beliefs}}},
  shorttitle = {Why {{Education Predicts Decreased Belief}} in {{Conspiracy Theories}}},
  author = {Van Prooijen, Jan-Willem},
  date = {2017-01},
  journaltitle = {Applied Cognitive Psychology},
  shortjournal = {Appl. Cognit. Psychol.},
  volume = {31},
  number = {1},
  pages = {50--58},
  issn = {08884080},
  doi = {10.1002/acp.3301},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/acp.3301},
  urldate = {2023-08-26},
  abstract = {People with high education are less likely than people with low education to believe in conspiracy theories. It is yet unclear why these effects occur, however, as education predicts a range of cognitive, emotional, and social outcomes. The present research sought to identify mediators of the relationship between education and conspiracy beliefs. Results of Study 1 revealed three independent mediators of this relationship, namely, belief in simple solutions for complex problems, feelings of powerlessness, and subjective social class. A nationally representative sample (Study 2) replicated these findings except for subjective social class. Moreover, variations in analytic thinking statistically accounted for the path through belief in simple solutions. I conclude that the relationship between education and conspiracy beliefs cannot be reduced to a single mechanism but is the result of the complex interplay of multiple psychological factors that are associated with education. © 2016 The Authors. Applied Cognitive Psychology published by John Wiley \& Sons Ltd.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\CRZ27USV\Van Prooijen - 2017 - Why Education Predicts Decreased Belief in Conspir.pdf}
}

@article{wahCaltechUCSDBirds2002011Dataset2011,
  title = {The {{Caltech-UCSD Birds-200-2011 Dataset}}},
  author = {Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  date = {2011},
  url = {https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf},
  abstract = {CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and attribute labels. Images and annotations were filtered by multiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\YLEE9VEC\Wah et al. - The Caltech-UCSD Birds-200-2011 Dataset.pdf}
}

@article{waltersFabricationErrorsBibliographic2023,
  title = {Fabrication and Errors in the Bibliographic Citations Generated by {{ChatGPT}}},
  author = {Walters, William H. and Wilder, Esther Isabelle},
  date = {2023-09-07},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {13},
  number = {1},
  pages = {14045},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-41032-5},
  url = {https://www.nature.com/articles/s41598-023-41032-5},
  urldate = {2023-09-12},
  abstract = {Abstract             Although chatbots such as ChatGPT can facilitate cost-effective text generation and editing, factually incorrect responses (hallucinations) limit their utility. This study evaluates one particular type of hallucination: fabricated bibliographic citations that do not represent actual scholarly works. We used ChatGPT-3.5 and ChatGPT-4 to produce short literature reviews on 42 multidisciplinary topics, compiling data on the 636 bibliographic citations (references) found in the 84 papers. We then searched multiple databases and websites to determine the prevalence of fabricated citations, to identify errors in the citations to non-fabricated papers, and to evaluate adherence to APA citation format. Within this set of documents, 55\% of the GPT-3.5 citations but just 18\% of the GPT-4 citations are fabricated. Likewise, 43\% of the real (non-fabricated) GPT-3.5 citations but just 24\% of the real GPT-4 citations include substantive citation errors. Although GPT-4 is a major improvement over GPT-3.5, problems remain.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\9ZF69K3D\Walters and Wilder - 2023 - Fabrication and errors in the bibliographic citati.pdf}
}

@article{walterUsing7pointChecklist2013,
  title = {Using the 7-Point Checklist as a Diagnostic Aid for Pigmented Skin Lesions in General Practice: A Diagnostic Validation Study},
  shorttitle = {Using the 7-Point Checklist as a Diagnostic Aid for Pigmented Skin Lesions in General Practice},
  author = {Walter, Fiona M and Prevost, A Toby and Vasconcelos, Joana and Hall, Per N and Burrows, Nigel P and Morris, Helen C and Kinmonth, Ann Louise and Emery, Jon D},
  date = {2013-05},
  journaltitle = {British Journal of General Practice},
  shortjournal = {Br J Gen Pract},
  volume = {63},
  number = {610},
  pages = {e345-e353},
  issn = {0960-1643, 1478-5242},
  doi = {10.3399/bjgp13X667213},
  url = {https://bjgp.org/lookup/doi/10.3399/bjgp13X667213},
  urldate = {2023-10-06},
  abstract = {Background GPs need to recognise significant pigmented skin lesions, given rising UK incidence rates for malignant melanoma. The 7-point checklist (7PCL) has been recommended by NICE (2005) for routine use in UK general practice to identify clinically significant lesions which require urgent referral. Aim To validate the Original and Weighted versions of the 7PCL in the primary care setting. Design and setting Diagnostic validation study, using data from a SIAscopic diagnostic aid randomised controlled trial in eastern England. Method Adults presenting in general practice with a pigmented skin lesion that could not be immediately diagnosed as benign were recruited into the trial. Reference standard diagnoses were histology or dermatology expert opinion; 7PCL scores were calculated blinded to the reference diagnosis. A case was defined as a clinically significant lesion for primary care referral to secondary care (total 1436 lesions: 225 cases, 1211 controls); or melanoma (36). Results For diagnosing clinically significant lesions there was a difference between the performance of the Original and Weighted 7PCLs (respectively, area under curve: 0.66, 0.69, difference = 0.03, P{$<$}0.001). For the identification of melanoma, similar differences were found. Increasing the Weighted 7PCL’s cut-off score from recommended 3 to 4 improved detection of clinically significant lesions in primary care: sensitivity 73.3\%, specificity 57.1\%, positive predictive value 24.1\%, negative predictive value 92.0\%, while maintaining high sensitivity of 91.7\% and moderate specificity of 53.4\% for melanoma. Conclusion The Original and Weighted 7PCLs both performed well in a primary care setting to identify clinically significant lesions as well as melanoma. The Weighted 7PCL, with a revised cut-off score of 4 from 3, performs slightly better and could be applied in general practice to support the recognition of clinically significant lesions and therefore the early identification of melanoma.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\UW5SZBRP\Walter et al. - 2013 - Using the 7-point checklist as a diagnostic aid fo.pdf}
}

@online{wangChatCADInteractiveComputerAided2023,
  title = {{{ChatCAD}}: {{Interactive Computer-Aided Diagnosis}} on {{Medical Image}} Using {{Large Language Models}}},
  shorttitle = {{{ChatCAD}}},
  author = {Wang, Sheng and Zhao, Zihao and Ouyang, Xi and Wang, Qian and Shen, Dinggang},
  date = {2023-02-14},
  eprint = {2302.07257},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2302.07257},
  urldate = {2023-10-04},
  abstract = {Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\c21012241\Zotero\storage\JSMDTDRE\Wang et al. - 2023 - ChatCAD Interactive Computer-Aided Diagnosis on M.pdf}
}

@online{wellerTransparencyMotivationsChallenges2019,
  title = {Transparency: {{Motivations}} and {{Challenges}}},
  shorttitle = {Transparency},
  author = {Weller, Adrian},
  date = {2019-08-19},
  eprint = {1708.01870},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.01870},
  urldate = {2023-10-10},
  abstract = {Transparency is often deemed critical to enable effective realworld deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns, particularly when agents have misaligned interests.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society},
  file = {C:\Users\c21012241\Zotero\storage\Z9SGU4RB\Weller - 2019 - Transparency Motivations and Challenges.pdf}
}

@online{wiggersNewYorkCity2018,
  title = {New {{York City}} Announces Task Force to Find Biases in Algorithms},
  author = {Wiggers, K.},
  date = {2018-05-16T23:09:00+00:00},
  url = {https://venturebeat.com/ai/new-york-city-announces-task-force-to-find-biases-in-algorithms/},
  urldate = {2023-08-24},
  abstract = {Head over to our on-demand library to view sessions from VB Transform 2023. Register Here New York City mayor Bill De Blasio today announced a task force to examine how city agencies use algorithms to make decisions. The mayor’s office called it the “first of its kind in the U.S.” and said the task force~[…]},
  langid = {american},
  organization = {{VentureBeat}},
  file = {C:\Users\c21012241\Zotero\storage\U8W6LIRW\new-york-city-announces-task-force-to-find-biases-in-algorithms.html}
}

@misc{worldeconomicforumPresidioRecommendationsResponsible2023,
  title = {The {{Presidio Recommendations}} on {{Responsible Generative AI}}},
  author = {World Economic Forum},
  date = {2023-06},
  url = {https://www3.weforum.org/docs/WEF_Presidio_Recommendations_on_Responsible_Generative_AI_2023.pdf},
  file = {C:\Users\c21012241\Zotero\storage\W2SLTBJF\WEF_Presidio_Recommendations_on_Responsible_Generative_AI_2023.pdf}
}

@online{wuNExTGPTAnytoAnyMultimodal2023,
  title = {{{NExT-GPT}}: {{Any-to-Any Multimodal LLM}}},
  shorttitle = {{{NExT-GPT}}},
  author = {Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  date = {2023-09-13},
  eprint = {2309.05519},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.05519},
  urldate = {2023-10-03},
  abstract = {While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1\%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building a unified AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\U9HQHEA3\Wu et al. - 2023 - NExT-GPT Any-to-Any Multimodal LLM.pdf}
}

@inproceedings{yangLanguageBottleLanguage2023,
  title = {Language in a {{Bottle}}: {{Language Model Guided Concept Bottlenecks}} for {{Interpretable Image Classification}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}), 2023},
  author = {Yang, Yue and Panagopoulou, Artemis and Zhou, Shenghao and Jin, Daniel and Callison-Burch, Chris and Yatskar, Mark},
  date = {2023},
  doi = {10.48550/arXiv.2211.11158},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\6ZKU66B3\Yang et al. - Language in a Bottle Language Model Guided Concep.pdf}
}

@article{yangSurveyExplainableAI2023,
  title = {Survey on {{Explainable AI}}: {{From Approaches}}, {{Limitations}} and {{Applications Aspects}}},
  shorttitle = {Survey on {{Explainable AI}}},
  author = {Yang, Wenli and Wei, Yuchen and Wei, Hanyu and Chen, Yanyu and Huang, Guan and Li, Xiang and Li, Renjie and Yao, Naimeng and Wang, Xinyi and Gu, Xiaotong and Amin, Muhammad Bilal and Kang, Byeong},
  date = {2023-08-10},
  journaltitle = {Human-Centric Intelligent Systems},
  shortjournal = {Hum-Cent Intell Syst},
  volume = {3},
  number = {3},
  pages = {161--188},
  issn = {2667-1336},
  doi = {10.1007/s44230-023-00038-y},
  url = {https://link.springer.com/10.1007/s44230-023-00038-y},
  urldate = {2023-10-06},
  abstract = {In recent years, artificial intelligence (AI) technology has been used in most if not all domains and has greatly benefited our lives. While AI can accurately extract critical features and valuable information from large amounts of data to help people complete tasks faster, there are growing concerns about the non-transparency of AI in the decision-making process. The emergence of explainable AI (XAI) has allowed humans to better understand and control AI systems, which is motivated to provide transparent explanations for the decisions made by AI. This article aims to present a comprehensive overview of recent research on XAI approaches from three well-defined taxonomies. We offer an in-depth analysis and summary of the status and prospects of XAI applications in several key areas where reliable explanations are urgently needed to avoid mistakes in decision-making. We conclude by discussing XAI’s limitations and future research directions.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\DCK8HM5K\Yang et al. - 2023 - Survey on Explainable AI From Approaches, Limitat.pdf}
}

@inproceedings{yehCompletenessawareConceptBasedExplanations2019,
  title = {On {{Completeness-aware Concept-Based Explanations}} in {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33 ({{NeurIPS}} 2020)},
  author = {Yeh, Chih-Kuan and Kim, Been and Arık, Sercan Ö and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
  date = {2019},
  doi = {10.48550/arXiv.1910.07969},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ecb287ff763c169694f682af52c1f309-Paper.pdf},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\2WNE4T3U\Yeh et al. - On Completeness-aware Concept-Based Explanations i.pdf}
}

@online{yuksekgonulPosthocConceptBottleneck2022,
  title = {Post-Hoc {{Concept Bottleneck Models}}},
  author = {Yuksekgonul, Mert and Wang, Maggie and Zou, James},
  date = {2022},
  eprint = {2205.15480},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2205.15480},
  urldate = {2023-08-26},
  abstract = {Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (“the bottleneck”) and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model ”sees” in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via conceptlevel feedback can provide significant performance gains without using data from the target domain or model retraining. The code for our paper can be found in https://github.com/mertyg/post-hoc-cbm.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\5QA8NQGI\Yuksekgonul et al. - 2023 - Post-hoc Concept Bottleneck Models.pdf}
}

@incollection{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-10590-1_53},
  url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
  urldate = {2023-10-10},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  langid = {english}
}

@article{zhangArtificialIntelligenceAmerican2019,
  title = {Artificial {{Intelligence}}: {{American Attitudes}} and {{Trends}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Zhang, Baobao and Dafoe, Allan},
  date = {2019},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3312874},
  url = {https://www.ssrn.com/abstract=3312874},
  urldate = {2023-10-09},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\V8WK32T5\Zhang and Dafoe - 2019 - Artificial Intelligence American Attitudes and Tr.pdf}
}

@online{zhangMayAskFollowup2023,
  title = {May {{I Ask}} a {{Follow-up Question}}? {{Understanding}} the {{Benefits}} of {{Conversations}} in {{Neural Network Explainability}}},
  shorttitle = {May {{I Ask}} a {{Follow-up Question}}?},
  author = {Zhang, Tong and Yang, X. Jessie and Li, Boyang},
  date = {2023-09-25},
  eprint = {2309.13965},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.13965},
  urldate = {2023-10-03},
  abstract = {Research in explainable AI (XAI) aims to provide insights into the decision-making process of opaque AI models. To date, most XAI methods offer one-off and static explanations, which cannot cater to the diverse backgrounds and understanding levels of users. With this paper, we investigate if free-form conversations can enhance users’ comprehension of static explanations, improve acceptance and trust in the explanation methods, and facilitate human-AI collaboration. Participants are presented with static explanations, followed by a conversation with a human expert regarding the explanations. We measure the effect of the conversation on participants’ ability to choose, from three machine learning models, the most accurate one based on explanations and their self-reported comprehension, acceptance, and trust. Empirical results show that conversations significantly improve comprehension, acceptance, trust, and collaboration. Our findings highlight the importance of customized model explanations in the format of free-form conversations and provide insights for the future design of conversational explanations.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {C:\Users\c21012241\Zotero\storage\VPKLZTNM\Zhang et al. - 2023 - May I Ask a Follow-up Question Understanding the .pdf}
}

@article{zhangPublicOpinionArtificial2022,
  title = {Public {{Opinion}} toward {{Artificial Intelligence}}},
  author = {Zhang, Baobao},
  date = {2022},
  abstract = {This chapter synthesizes and discusses research on public opinion toward arti cial intelligence (AI). Understanding citizens’ and consumers’ attitudes toward AI is important from a normative standpoint because the public is a major stakeholder in shaping the future of the technology and should have a voice in policy discussions. Furthermore, the research could help anticipate future political and consumer behavior. Survey data worldwide show that the public is increasingly aware of AI; however, they—unlike AI researchers—tend to anthropomorphize AI. Demographic di erences correlate with trust in AI in general: those living in East Asia have higher levels of trust in AI, while women and those of lower socioeconomic status across di erent regions have lower levels of trust. Surveys that focus on particular AI applications, including facial recognition technology, personalization algorithms, lethal autonomous weapons, and workplace automation, add complexity to this research topic. This chapter concludes by recommending four new topics for future studies: (1) institutional trust in actors building and deploying AI systems, (2) the impact of knowledge and experience on attitudes toward AI, (3) heterogeneity in attitudes toward AI, and (4) the relationship between attitudes and behavior.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\BP8LZZB9\Zhang - Public Opinion toward Artificial Intelligence.pdf}
}

@online{zhaoExplainabilityLargeLanguage2023,
  title = {Explainability for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Explainability for {{Large Language Models}}},
  author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  date = {2023-09-16},
  eprint = {2309.01029},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.01029},
  urldate = {2023-10-09},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\7WDNM3WR\Zhao et al. - 2023 - Explainability for Large Language Models A Survey.pdf}
}

@online{zhaoSurveyLargeLanguage2023,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  date = {2023-06-29},
  eprint = {2303.18223},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.18223},
  urldate = {2023-08-28},
  abstract = {Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\c21012241\Zotero\storage\E8HQABXM\Zhao et al. - 2023 - A Survey of Large Language Models.pdf}
}

@incollection{zhouInterpretableBasisDecomposition2018,
  title = {Interpretable {{Basis Decomposition}} for {{Visual Explanation}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba, Antonio},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  volume = {11212},
  pages = {122--138},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01237-3_8},
  url = {https://link.springer.com/10.1007/978-3-030-01237-3_8},
  urldate = {2023-08-24},
  abstract = {Explanations of the decisions made by a deep neural network are important for human end-users to be able to understand and diagnose the trustworthiness of the system. Current neural networks used for visual recognition are generally used as black boxes that do not provide any human interpretable justification for a prediction. In this work we propose a new framework called Interpretable Basis Decomposition for providing visual explanations for classification networks. By decomposing the neural activations of the input image into semantically interpretable components pre-trained from a large concept corpus, the proposed framework is able to disentangle the evidence encoded in the activation feature vector, and quantify the contribution of each piece of evidence to the final prediction. We apply our framework for providing explanations to several popular networks for visual recognition, and show it is able to explain the predictions given by the networks in a humaninterpretable way. The human interpretability of the visual explanations provided by our framework and other recent explanation methods is evaluated through Amazon Mechanical Turk, showing that our framework generates more faithful and interpretable explanations1.},
  isbn = {978-3-030-01236-6 978-3-030-01237-3},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\34EMSJL4\Zhou et al. - 2018 - Interpretable Basis Decomposition for Visual Expla.pdf}
}

@article{zhouInterpretingDeepVisual2017,
  title = {Interpreting {{Deep Visual Representations}} via {{Network Dissection}}},
  author = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
  date = {2017},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {41},
  number = {9},
  pages = {2131--2145},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2018.2858759},
  url = {https://ieeexplore.ieee.org/document/8417924/},
  urldate = {2023-08-18},
  abstract = {The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. In this work, we describe Network Dissection, a method that interprets networks by providing meaningful labels to their individual units. The proposed method quantifies the interpretability of CNN representations by evaluating the alignment between individual hidden units and visual semantic concepts. By identifying the best alignments, units are given interpretable labels ranging from colors, materials, textures, parts, objects and scenes. The method reveals that deep representations are more transparent and interpretable than they would be under a random equivalently powerful basis. We apply our approach to interpret and compare the latent representations of several network architectures trained to solve a wide range of supervised and self-supervised tasks. We then examine factors affecting the network interpretability such as the number of the training iterations, regularizations, different initialization parameters, as well as networks depth and width. Finally we show that the interpreted units can be used to provide explicit explanations of a given CNN prediction for an image. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into what hierarchical structures can learn.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\K7UKNP6E\Zhou et al. - 2019 - Interpreting Deep Visual Representations via Netwo.pdf}
}

@article{zhouPlaces10Million2018,
  title = {Places: {{A}} 10 {{Million Image Database}} for {{Scene Recognition}}},
  shorttitle = {Places},
  author = {Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  date = {2018-06-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {40},
  number = {6},
  pages = {1452--1464},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2017.2723009},
  url = {https://ieeexplore.ieee.org/document/7968387/},
  urldate = {2023-08-22},
  abstract = {The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\6MK2XZKV\Zhou et al. - 2018 - Places A 10 Million Image Database for Scene Reco.pdf}
}

@online{zhouRevisitingImportanceIndividual2018,
  title = {Revisiting the {{Importance}} of {{Individual Units}} in {{CNNs}} via {{Ablation}}},
  author = {Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba, Antonio},
  date = {2018-06-07},
  eprint = {1806.02891},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1806.02891},
  urldate = {2023-08-22},
  abstract = {We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classification accuracy, it does lead to significant damage on the accuracy of specific classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We confirm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work [11]. However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to specific classes. Our results show that units with high selectivity play an important role in network classification power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\c21012241\Zotero\storage\LP2ZPMU7\Zhou et al. - 2018 - Revisiting the Importance of Individual Units in C.pdf}
}

@article{zimmermannPoliticalPhilosophyData2022,
  title = {The {{Political Philosophy}} of {{Data}} and {{AI}}},
  author = {Zimmermann, Annette and Vredenburgh, Kate and Lazar, Seth},
  date = {2022-01},
  journaltitle = {Canadian Journal of Philosophy},
  shortjournal = {Can. J. of Philosophy},
  volume = {52},
  number = {1},
  pages = {1--5},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2022.28},
  url = {https://www.cambridge.org/core/product/identifier/S0045509122000285/type/journal_article},
  urldate = {2023-10-09},
  abstract = {We are increasingly subject to the power of technological systems relying on big data and AI. These systems are reshaping the welfare state and the administration of criminal justice. They are used to police tax evasion, track down child abusers, and model the spread of the pandemic. And they are used to weaponize vast surveillance networks through facial recognition technology. But algorithmic power extends far beyond the state: we spend ever more time working, socialising, and consuming within digital platforms. Our experiences are governed by algorithms that are constantly monitoring and shaping our behaviour and our attention, automatically selecting what we do and do not see. These online experiences have offline consequences, among them an unprecedented challenge to democratic processes worldwide.},
  langid = {english},
  file = {C:\Users\c21012241\Zotero\storage\RMQS6E9L\Zimmermann et al. - 2022 - The Political Philosophy of Data and AI.pdf}
}
